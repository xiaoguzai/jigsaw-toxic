{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cdd8d45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:19:36.698039Z",
     "iopub.status.busy": "2021-12-26T11:19:36.697279Z",
     "iopub.status.idle": "2021-12-26T11:19:37.635223Z",
     "shell.execute_reply": "2021-12-26T11:19:37.634469Z",
     "shell.execute_reply.started": "2021-12-25T10:09:56.765843Z"
    },
    "papermill": {
     "duration": 0.971018,
     "end_time": "2021-12-26T11:19:37.635380",
     "exception": false,
     "start_time": "2021-12-26T11:19:36.664362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6643bf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:19:37.678171Z",
     "iopub.status.busy": "2021-12-26T11:19:37.677397Z",
     "iopub.status.idle": "2021-12-26T11:19:37.679745Z",
     "shell.execute_reply": "2021-12-26T11:19:37.679338Z",
     "shell.execute_reply.started": "2021-12-25T10:09:57.665612Z"
    },
    "papermill": {
     "duration": 0.02487,
     "end_time": "2021-12-26T11:19:37.679854",
     "exception": false,
     "start_time": "2021-12-26T11:19:37.654984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from joblib import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be79f18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:19:37.721833Z",
     "iopub.status.busy": "2021-12-26T11:19:37.721297Z",
     "iopub.status.idle": "2021-12-26T11:19:38.330681Z",
     "shell.execute_reply": "2021-12-26T11:19:38.330118Z",
     "shell.execute_reply.started": "2021-12-25T10:09:57.675169Z"
    },
    "papermill": {
     "duration": 0.632027,
     "end_time": "2021-12-26T11:19:38.330875",
     "exception": false,
     "start_time": "2021-12-26T11:19:37.698848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_val = pd.read_csv('/kaggle/input/jigsaw-toxic-severity-rating/validation_data.csv')\n",
    "df_sub = pd.read_csv(\"/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5474b81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:19:38.375424Z",
     "iopub.status.busy": "2021-12-26T11:19:38.374875Z",
     "iopub.status.idle": "2021-12-26T11:19:38.377599Z",
     "shell.execute_reply": "2021-12-26T11:19:38.377183Z",
     "shell.execute_reply.started": "2021-12-25T10:09:58.273837Z"
    },
    "papermill": {
     "duration": 0.027272,
     "end_time": "2021-12-26T11:19:38.377713",
     "exception": false,
     "start_time": "2021-12-26T11:19:38.350441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_folds = 7\n",
    "val_preds_arr1 = np.zeros((df_val.shape[0], n_folds))\n",
    "val_preds_arr2 = np.zeros((df_val.shape[0], n_folds))\n",
    "test_preds_arr = np.zeros((df_sub.shape[0], n_folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d77661f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:19:38.423296Z",
     "iopub.status.busy": "2021-12-26T11:19:38.422669Z",
     "iopub.status.idle": "2021-12-26T11:25:23.354361Z",
     "shell.execute_reply": "2021-12-26T11:25:23.353767Z",
     "shell.execute_reply.started": "2021-12-25T10:09:58.281592Z"
    },
    "papermill": {
     "duration": 344.957639,
     "end_time": "2021-12-26T11:25:23.354498",
     "exception": false,
     "start_time": "2021-12-26T11:19:38.396859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of features: 75032\n",
      "[('vect3__uck', 0.39),\n",
      " ('vect3__ f ', 0.36),\n",
      " ('vect3__fuc', 0.32),\n",
      " ('vect3__fuck', 0.31),\n",
      " ('vect3__shit', 0.31),\n",
      " ('vect3__ ass', 0.28),\n",
      " ('vect3__ass', 0.28),\n",
      " ('vect3__hit', 0.28),\n",
      " ('vect3__ rape', 0.27),\n",
      " ('vect3__ fu', 0.26),\n",
      " ('vect3__ nl', 0.26),\n",
      " ('vect3__nl3', 0.26),\n",
      " ('vect3__ g ', 0.24),\n",
      " ('vect3__nig', 0.24),\n",
      " ('vect3__bitch', 0.23),\n",
      " ('vect3__fag', 0.23),\n",
      " ('vect3__suck', 0.23),\n",
      " ('vect3__ fuc', 0.22),\n",
      " ('vect3__ gay', 0.22),\n",
      " ('vect3__ rap', 0.22),\n",
      " ('vect3__ shit', 0.22),\n",
      " ('vect3__rape ', 0.22),\n",
      " ('vect3__ fag', 0.21),\n",
      " ('vect3__ fuck', 0.21),\n",
      " ('vect3__bitc', 0.21),\n",
      " ('vect3__nigg', 0.21),\n",
      " ('vect3__ * ', 0.2),\n",
      " ('vect3__ as', 0.2),\n",
      " ('vect3__ nig', 0.2),\n",
      " ('vect3__ie ', 0.2)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "Total number of features: 74635\n",
      "[('vect3__uck', 0.42),\n",
      " ('vect3__fuc', 0.33),\n",
      " ('vect3__fuck', 0.3),\n",
      " ('vect3__ f ', 0.29),\n",
      " ('vect3__ fu', 0.29),\n",
      " ('vect3__ nl', 0.29),\n",
      " ('vect3__ck ', 0.27),\n",
      " ('vect3__ie ', 0.27),\n",
      " ('vect3__l33', 0.27),\n",
      " ('vect3__ die ', 0.26),\n",
      " ('vect3__die ', 0.25),\n",
      " ('vect3__fag', 0.25),\n",
      " ('vect3__ ass', 0.24),\n",
      " ('vect3__ ck', 0.24),\n",
      " ('vect3__ die', 0.24),\n",
      " ('vect3__ass', 0.24),\n",
      " ('vect3__hit', 0.24),\n",
      " ('vect3__ fag', 0.23),\n",
      " ('vect3__fuk', 0.23),\n",
      " ('vect3__nig', 0.23),\n",
      " ('vect3__shit', 0.23),\n",
      " ('vect3__suck', 0.23),\n",
      " ('vect3__ as', 0.22),\n",
      " ('vect3__ fuc', 0.22),\n",
      " ('vect3__ gay', 0.22),\n",
      " ('vect3__gay', 0.22),\n",
      " ('vect3__nigg', 0.22),\n",
      " ('vect3__ fuk', 0.21),\n",
      " ('vect3__bag', 0.21),\n",
      " ('vect3__ nig', 0.2)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "Total number of features: 74452\n",
      "[('vect3__uck', 0.42),\n",
      " ('vect3__ f ', 0.32),\n",
      " ('vect3__fuc', 0.32),\n",
      " ('vect3__///', 0.31),\n",
      " ('vect3__fuck', 0.31),\n",
      " ('vect3__shit', 0.25),\n",
      " ('vect3__ ass', 0.24),\n",
      " ('vect3__ass', 0.24),\n",
      " ('vect3__fucke', 0.24),\n",
      " ('vect3__hit', 0.24),\n",
      " ('vect3__igg', 0.24),\n",
      " ('vect3__ fu', 0.23),\n",
      " ('vect3__ck ', 0.23),\n",
      " ('vect3__die ', 0.23),\n",
      " ('vect3__nigg', 0.23),\n",
      " ('vect3__ a ', 0.22),\n",
      " ('vect3__ gay', 0.22),\n",
      " ('vect3__suck', 0.22),\n",
      " ('vect3__ucke', 0.22),\n",
      " ('vect3__ fuc', 0.21),\n",
      " ('vect3__ je', 0.21),\n",
      " ('vect3__bitc', 0.21),\n",
      " ('vect3__dick', 0.21),\n",
      " ('vect3__ ass.', 0.2),\n",
      " ('vect3__ ck', 0.2),\n",
      " ('vect3__ fuck', 0.2),\n",
      " ('vect3__ass.', 0.2),\n",
      " ('vect3__bitch', 0.2),\n",
      " ('vect3__ bitc', 0.19),\n",
      " ('vect3__ gay ', 0.19)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "Total number of features: 75816\n",
      "[('vect3__uck', 0.41),\n",
      " ('vect3__fuc', 0.33),\n",
      " ('vect3__ f ', 0.31),\n",
      " ('vect3__fuck', 0.31),\n",
      " ('vect3__ fu', 0.29),\n",
      " ('vect3__ ass', 0.27),\n",
      " ('vect3__ fag', 0.25),\n",
      " ('vect3__fag', 0.24),\n",
      " ('vect3__hit', 0.24),\n",
      " ('vect3__ nl', 0.23),\n",
      " ('vect3__ass', 0.23),\n",
      " ('vect3__ck ', 0.23),\n",
      " ('vect3__gay', 0.23),\n",
      " ('vect3__shit', 0.23),\n",
      " ('vect3__ as', 0.22),\n",
      " ('vect3__ gay', 0.22),\n",
      " ('vect3__ die', 0.21),\n",
      " ('vect3__ nig', 0.21),\n",
      " ('vect3__bitc', 0.21),\n",
      " ('vect3__fuk', 0.21),\n",
      " ('vect3__ker', 0.21),\n",
      " ('vect3__nig', 0.21),\n",
      " ('vect3__ fuk', 0.2),\n",
      " ('vect3__ rap', 0.2),\n",
      " ('vect3__ u ', 0.2),\n",
      " ('vect3__=====', 0.2),\n",
      " ('vect3__bitch', 0.2),\n",
      " ('vect3__cun', 0.2),\n",
      " ('vect3__cunt', 0.2),\n",
      " ('vect3__diot', 0.2)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "Total number of features: 74327\n",
      "[('vect3__uck', 0.42),\n",
      " ('vect3__ f ', 0.38),\n",
      " ('vect3__fuc', 0.33),\n",
      " ('vect3__ ass', 0.32),\n",
      " ('vect3__fag', 0.31),\n",
      " ('vect3__fuck', 0.3),\n",
      " ('vect3__ nl', 0.27),\n",
      " ('vect3__ u ', 0.27),\n",
      " ('vect3__ass', 0.26),\n",
      " ('vect3__ck ', 0.26),\n",
      " ('vect3__shit', 0.26),\n",
      " ('vect3__ fag', 0.25),\n",
      " ('vect3__gay', 0.25),\n",
      " ('vect3__hit', 0.25),\n",
      " ('vect3__ ck', 0.24),\n",
      " ('vect3__ fu', 0.24),\n",
      " ('vect3__suck', 0.23),\n",
      " ('vect3__ ass ', 0.22),\n",
      " ('vect3__diot', 0.22),\n",
      " ('vect3__nig', 0.22),\n",
      " ('vect3__ as', 0.21),\n",
      " ('vect3__ fuc', 0.21),\n",
      " ('vect3__ gay', 0.21),\n",
      " ('vect3__cun', 0.21),\n",
      " ('vect3__idio', 0.21),\n",
      " ('vect3__iot', 0.21),\n",
      " ('vect3__nigg', 0.21),\n",
      " ('vect3__ rape', 0.2),\n",
      " ('vect3__cunt', 0.2),\n",
      " ('vect3__dio', 0.2)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "Total number of features: 75102\n",
      "[('vect3__uck', 0.42),\n",
      " ('vect3__ f ', 0.35),\n",
      " ('vect3__fuc', 0.33),\n",
      " ('vect3__ fu', 0.32),\n",
      " ('vect3__///', 0.31),\n",
      " ('vect3__gay', 0.31),\n",
      " ('vect3__fuck', 0.3),\n",
      " ('vect3__ ass', 0.28),\n",
      " ('vect3__ ck', 0.28),\n",
      " ('vect3__fuk', 0.27),\n",
      " ('vect3__ c ', 0.25),\n",
      " ('vect3__ gay', 0.25),\n",
      " ('vect3__ k ', 0.25),\n",
      " ('vect3__ass', 0.25),\n",
      " ('vect3__nig', 0.25),\n",
      " ('vect3__shit', 0.25),\n",
      " ('vect3__ die', 0.24),\n",
      " ('vect3__die', 0.24),\n",
      " ('vect3__die ', 0.24),\n",
      " ('vect3__ dick', 0.23),\n",
      " ('vect3__ nl', 0.23),\n",
      " ('vect3__ie ', 0.23),\n",
      " ('vect3__ cunt', 0.22),\n",
      " ('vect3__ fuk', 0.22),\n",
      " ('vect3__cunt', 0.22),\n",
      " ('vect3__dick', 0.22),\n",
      " ('vect3__ cun', 0.21),\n",
      " ('vect3__ di', 0.21),\n",
      " ('vect3__ die ', 0.21),\n",
      " ('vect3__ nig', 0.21)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "Total number of features: 74475\n",
      "[('vect3__ f ', 0.37),\n",
      " ('vect3__uck', 0.37),\n",
      " ('vect3__fuc', 0.35),\n",
      " ('vect3__ fu', 0.32),\n",
      " ('vect3__fuck', 0.32),\n",
      " ('vect3__ ass', 0.3),\n",
      " ('vect3__ck ', 0.29),\n",
      " ('vect3__ ck', 0.28),\n",
      " ('vect3__fag', 0.28),\n",
      " ('vect3__shit', 0.28),\n",
      " ('vect3__ass', 0.26),\n",
      " ('vect3__gay', 0.25),\n",
      " ('vect3__ fag', 0.23),\n",
      " ('vect3__ fuc', 0.23),\n",
      " ('vect3__ nl', 0.23),\n",
      " ('vect3__hit', 0.23),\n",
      " ('vect3__ shit', 0.22),\n",
      " ('vect3__ass.', 0.22),\n",
      " ('vect3__ ass ', 0.21),\n",
      " ('vect3__ k ', 0.21),\n",
      " ('vect3__dick', 0.21),\n",
      " ('vect3__ ass.', 0.2),\n",
      " ('vect3__ gay', 0.2),\n",
      " ('vect3__ rape', 0.2),\n",
      " ('vect3__bitc', 0.2),\n",
      " ('vect3__bitch', 0.2),\n",
      " ('vect3__nig', 0.2),\n",
      " ('vect3__nigg', 0.2),\n",
      " ('vect3__ fa', 0.19),\n",
      " ('vect3__ fuck', 0.19)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n"
     ]
    }
   ],
   "source": [
    "for fld in range(n_folds):\n",
    "    pipeline = load('/kaggle/input/toxiclinearregression/withclean'+str(fld)+'.joblib')\n",
    "    \n",
    "    # What are the important features for toxicity\n",
    "\n",
    "    print('\\nTotal number of features:', len(pipeline['features'].get_feature_names()) )\n",
    "\n",
    "    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n",
    "                                  np.round(pipeline['clf'].coef_,2) )), \n",
    "                         key = lambda x:x[1], \n",
    "                         reverse=True)\n",
    "\n",
    "    pprint(feature_wts[:30])\n",
    "    \n",
    "    print(\"\\npredict validation data \")\n",
    "    val_preds_arr1[:,fld] = pipeline.predict(df_val['less_toxic'])\n",
    "    val_preds_arr2[:,fld] = pipeline.predict(df_val['more_toxic'])\n",
    "    \n",
    "    print(\"\\npredict test data \")\n",
    "    test_preds_arr[:,fld] = pipeline.predict(df_sub['text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35b4ebb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:25:23.406206Z",
     "iopub.status.busy": "2021-12-26T11:25:23.405338Z",
     "iopub.status.idle": "2021-12-26T11:25:23.409155Z",
     "shell.execute_reply": "2021-12-26T11:25:23.409513Z",
     "shell.execute_reply.started": "2021-12-25T10:15:40.052959Z"
    },
    "papermill": {
     "duration": 0.031707,
     "end_time": "2021-12-26T11:25:23.409677",
     "exception": false,
     "start_time": "2021-12-26T11:25:23.377970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_preds_arr1c = np.zeros((df_val.shape[0], n_folds))\n",
    "val_preds_arr2c = np.zeros((df_val.shape[0], n_folds))\n",
    "test_preds_arrc = np.zeros((df_sub.shape[0], n_folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "195dfcfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:25:23.462303Z",
     "iopub.status.busy": "2021-12-26T11:25:23.461812Z",
     "iopub.status.idle": "2021-12-26T11:31:12.917931Z",
     "shell.execute_reply": "2021-12-26T11:31:12.917028Z",
     "shell.execute_reply.started": "2021-12-25T10:15:40.061641Z"
    },
    "papermill": {
     "duration": 349.485214,
     "end_time": "2021-12-26T11:31:12.918080",
     "exception": false,
     "start_time": "2021-12-26T11:25:23.432866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of features: 80874\n",
      "[('vect3__uck', 0.43),\n",
      " ('vect3__fuc', 0.36),\n",
      " ('vect3__fuck', 0.32),\n",
      " ('vect3__you ', 0.32),\n",
      " ('vect3__ ass', 0.31),\n",
      " ('vect3__ass', 0.3),\n",
      " ('vect3__shit', 0.3),\n",
      " ('vect3__ you ', 0.28),\n",
      " ('vect3__hit', 0.28),\n",
      " ('vect3__ fu', 0.27),\n",
      " ('vect3__ nl', 0.26),\n",
      " ('vect3__ f ', 0.25),\n",
      " ('vect3__ rape', 0.25),\n",
      " ('vect3__nl3', 0.25),\n",
      " ('vect3__ g ', 0.24),\n",
      " ('vect3__ u ', 0.24),\n",
      " ('vect3__bitch', 0.24),\n",
      " ('vect3__fag', 0.24),\n",
      " ('vect3__nig', 0.24),\n",
      " ('vect3__ fag', 0.23),\n",
      " ('vect3__ gay', 0.23),\n",
      " ('vect3__nigg', 0.23),\n",
      " ('vect3__ fuc', 0.22),\n",
      " ('vect3__ shit', 0.22),\n",
      " ('vect3__bitc', 0.22),\n",
      " ('vect3__suck', 0.22),\n",
      " ('vect3__uck ', 0.22),\n",
      " ('vect3__wwwww', 0.22),\n",
      " ('vect3__ rap', 0.21),\n",
      " ('vect3__!!!', 0.21)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "Total number of features: 80314\n",
      "[('vect3__uck', 0.46),\n",
      " ('vect3__fuc', 0.34),\n",
      " ('vect3__fuck', 0.3),\n",
      " ('vect3__ fu', 0.28),\n",
      " ('vect3__ nl', 0.28),\n",
      " ('vect3__l33', 0.28),\n",
      " ('vect3__ ass', 0.27),\n",
      " ('vect3__ die ', 0.27),\n",
      " ('vect3__ u ', 0.27),\n",
      " ('vect3__ck ', 0.27),\n",
      " ('vect3__ f ', 0.26),\n",
      " ('vect3__ass', 0.26),\n",
      " ('vect3__die ', 0.26),\n",
      " ('vect3__fag', 0.26),\n",
      " ('vect3__ie ', 0.26),\n",
      " ('vect3__suck', 0.26),\n",
      " ('vect3__ die', 0.25),\n",
      " ('vect3__nig', 0.25),\n",
      " ('vect3__ fag', 0.24),\n",
      " ('vect3__hit', 0.24),\n",
      " ('vect3__shit', 0.24),\n",
      " ('vect3__ fuc', 0.23),\n",
      " ('vect3__nigg', 0.23),\n",
      " ('vect3__ttt', 0.23),\n",
      " ('vect3__gay', 0.22),\n",
      " ('vect3__ gay', 0.21),\n",
      " ('vect3__ suck', 0.21),\n",
      " ('vect3__fuk', 0.21),\n",
      " ('vect3__kkk', 0.21),\n",
      " ('vect3__tttt', 0.21)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "Total number of features: 80000\n",
      "[('vect3__uck', 0.45),\n",
      " ('vect3__fuc', 0.33),\n",
      " ('vect3__fuck', 0.32),\n",
      " ('vect3__///', 0.29),\n",
      " ('vect3__ ass', 0.27),\n",
      " ('vect3__ass', 0.26),\n",
      " ('vect3__hit', 0.25),\n",
      " ('vect3__shit', 0.25),\n",
      " ('vect3__fucke', 0.24),\n",
      " ('vect3__nigg', 0.24),\n",
      " ('vect3__bitc', 0.23),\n",
      " ('vect3__suck', 0.23),\n",
      " ('vect3__ fu', 0.22),\n",
      " ('vect3__ gay', 0.22),\n",
      " ('vect3__!!!', 0.22),\n",
      " ('vect3__bitch', 0.22),\n",
      " ('vect3__die ', 0.22),\n",
      " ('vect3__igg', 0.22),\n",
      " ('vect3__ou ', 0.22),\n",
      " ('vect3__ bitc', 0.21),\n",
      " ('vect3__ die', 0.21),\n",
      " ('vect3__ fuc', 0.21),\n",
      " ('vect3__ je', 0.21),\n",
      " ('vect3__ck ', 0.21),\n",
      " ('vect3__nig', 0.21),\n",
      " ('vect3__ f*', 0.2),\n",
      " ('vect3__ gay ', 0.2),\n",
      " ('vect3__ass.', 0.2),\n",
      " ('vect3__dick', 0.2),\n",
      " ('vect3__gay', 0.2)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "Total number of features: 81750\n",
      "[('vect3__ you ', 0.51),\n",
      " ('vect3__uck', 0.45),\n",
      " ('vect3__fuc', 0.36),\n",
      " ('vect3__fuck', 0.32),\n",
      " ('vect3__ ass', 0.3),\n",
      " ('vect3__ fu', 0.3),\n",
      " ('vect3__ fag', 0.27),\n",
      " ('vect3__ u ', 0.27),\n",
      " ('vect3__ass', 0.26),\n",
      " ('vect3__fag', 0.26),\n",
      " ('vect3__hit', 0.25),\n",
      " ('vect3__ die', 0.24),\n",
      " ('vect3__ gay', 0.24),\n",
      " ('vect3__gay', 0.24),\n",
      " ('vect3__bitc', 0.23),\n",
      " ('vect3__shit', 0.23),\n",
      " ('vect3__ nl', 0.22),\n",
      " ('vect3__bitch', 0.22),\n",
      " ('vect3__ttt', 0.22),\n",
      " ('vect3__ f*', 0.21),\n",
      " ('vect3__ nig', 0.21),\n",
      " ('vect3__ck ', 0.21),\n",
      " ('vect3__diot', 0.21),\n",
      " ('vect3__fuk', 0.21),\n",
      " ('vect3__ker', 0.21),\n",
      " ('vect3__nig', 0.21),\n",
      " ('vect3__nigg', 0.21),\n",
      " ('vect3__ fuk', 0.2),\n",
      " ('vect3__ homo', 0.2),\n",
      " ('vect3__bag', 0.2)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "Total number of features: 79830\n",
      "[('vect3__uck', 0.43),\n",
      " ('vect3__ ass', 0.36),\n",
      " ('vect3__fuc', 0.36),\n",
      " ('vect3__fuck', 0.33),\n",
      " ('vect3__fag', 0.31),\n",
      " ('vect3__ u ', 0.29),\n",
      " ('vect3__ass', 0.29),\n",
      " ('vect3__ nl', 0.27),\n",
      " ('vect3__gay', 0.26),\n",
      " ('vect3__shit', 0.26),\n",
      " ('vect3__ fag', 0.25),\n",
      " ('vect3__hit', 0.25),\n",
      " ('vect3__uuu', 0.25),\n",
      " ('vect3__ f ', 0.24),\n",
      " ('vect3__nig', 0.24),\n",
      " ('vect3__ fu', 0.23),\n",
      " ('vect3__diot', 0.23),\n",
      " ('vect3__iot', 0.23),\n",
      " ('vect3__nigg', 0.23),\n",
      " ('vect3__suck', 0.23),\n",
      " ('vect3__ f*', 0.22),\n",
      " ('vect3__ gay', 0.22),\n",
      " ('vect3__cun', 0.22),\n",
      " ('vect3__cunt', 0.22),\n",
      " ('vect3__idio', 0.22),\n",
      " ('vect3__uuuu', 0.22),\n",
      " ('vect3__ ass ', 0.21),\n",
      " ('vect3__ fuc', 0.21),\n",
      " ('vect3__ shit', 0.21),\n",
      " ('vect3__ck ', 0.21)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "Total number of features: 80740\n",
      "[('vect3__uck', 0.44),\n",
      " ('vect3__fuc', 0.34),\n",
      " ('vect3__ fu', 0.33),\n",
      " ('vect3__ ass', 0.31),\n",
      " ('vect3__fuck', 0.31),\n",
      " ('vect3__///', 0.3),\n",
      " ('vect3__gay', 0.3),\n",
      " ('vect3__ass', 0.28),\n",
      " ('vect3__die', 0.27),\n",
      " ('vect3__nig', 0.27),\n",
      " ('vect3__ die', 0.26),\n",
      " ('vect3__wwwww', 0.26),\n",
      " ('vect3__ f*', 0.25),\n",
      " ('vect3__ nl', 0.25),\n",
      " ('vect3__ gay', 0.24),\n",
      " ('vect3__ u ', 0.24),\n",
      " ('vect3__!!!', 0.24),\n",
      " ('vect3__dick', 0.24),\n",
      " ('vect3__fuk', 0.24),\n",
      " ('vect3__ttt', 0.24),\n",
      " ('vect3__shit', 0.23),\n",
      " ('vect3__wwww', 0.23),\n",
      " ('vect3__ dick', 0.22),\n",
      " ('vect3__ nig', 0.22),\n",
      " ('vect3__cunt', 0.22),\n",
      " ('vect3__die ', 0.22),\n",
      " ('vect3__fag', 0.22),\n",
      " ('vect3__tttt', 0.22),\n",
      " ('vect3__ ass.', 0.21),\n",
      " ('vect3__ cun', 0.21)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "Total number of features: 80092\n",
      "[('vect3__uck', 0.39),\n",
      " ('vect3__ttt', 0.37),\n",
      " ('vect3__ ass', 0.36),\n",
      " ('vect3__fuc', 0.36),\n",
      " ('vect3__ fu', 0.33),\n",
      " ('vect3__ass', 0.32),\n",
      " ('vect3__fuck', 0.32),\n",
      " ('vect3__ f ', 0.29),\n",
      " ('vect3__ck ', 0.28),\n",
      " ('vect3__fag', 0.28),\n",
      " ('vect3__shit', 0.28),\n",
      " ('vect3__ f*', 0.27),\n",
      " ('vect3__ u ', 0.26),\n",
      " ('vect3__gay', 0.26),\n",
      " ('vect3__ fag', 0.25),\n",
      " ('vect3__ nl', 0.24),\n",
      " ('vect3__ou ', 0.24),\n",
      " ('vect3__ fuc', 0.23),\n",
      " ('vect3__ shit', 0.23),\n",
      " ('vect3__*k ', 0.23),\n",
      " ('vect3__dick', 0.23),\n",
      " ('vect3__hit', 0.23),\n",
      " ('vect3__ ass ', 0.22),\n",
      " ('vect3__ gay', 0.22),\n",
      " ('vect3__die', 0.22),\n",
      " ('vect3__nig', 0.22),\n",
      " ('vect3__you ', 0.22),\n",
      " ('vect3__ die', 0.21),\n",
      " ('vect3__ fa', 0.21),\n",
      " ('vect3__!! ', 0.21)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n"
     ]
    }
   ],
   "source": [
    "for fld in range(n_folds):\n",
    "    pipeline = load('/kaggle/input/toxiclinearregression/withoutclean'+str(fld)+'.joblib')\n",
    "    \n",
    "    # What are the important features for toxicity\n",
    "\n",
    "    print('\\nTotal number of features:', len(pipeline['features'].get_feature_names()) )\n",
    "\n",
    "    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n",
    "                                  np.round(pipeline['clf'].coef_,2) )), \n",
    "                         key = lambda x:x[1], \n",
    "                         reverse=True)\n",
    "\n",
    "    pprint(feature_wts[:30])\n",
    "    \n",
    "    print(\"\\npredict validation data \")\n",
    "    val_preds_arr1c[:,fld] = pipeline.predict(df_val['less_toxic'])\n",
    "    val_preds_arr2c[:,fld] = pipeline.predict(df_val['more_toxic'])\n",
    "    \n",
    "    print(\"\\npredict test data \")\n",
    "    test_preds_arrc[:,fld] = pipeline.predict(df_sub['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d886a64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:31:12.982236Z",
     "iopub.status.busy": "2021-12-26T11:31:12.981308Z",
     "iopub.status.idle": "2021-12-26T11:36:53.380989Z",
     "shell.execute_reply": "2021-12-26T11:36:53.380463Z",
     "shell.execute_reply.started": "2021-12-25T10:21:26.607764Z"
    },
    "papermill": {
     "duration": 340.434774,
     "end_time": "2021-12-26T11:36:53.381133",
     "exception": false,
     "start_time": "2021-12-26T11:31:12.946359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vect3__fuc', 0.59),\n",
      " ('vect3__fuck', 0.59),\n",
      " ('vect3__uck', 0.55),\n",
      " ('vect3__ fuc', 0.52),\n",
      " ('vect3__ fuck', 0.52),\n",
      " ('vect3__ fu', 0.42),\n",
      " ('vect3__shit', 0.4),\n",
      " ('vect3__hit', 0.37),\n",
      " ('vect3__ shit', 0.36),\n",
      " ('vect3__fuck ', 0.34),\n",
      " ('vect3__ck ', 0.32),\n",
      " ('vect3__uck ', 0.31),\n",
      " ('vect3__shi', 0.29),\n",
      " ('vect3__ shi', 0.28),\n",
      " ('vect3__ ass ', 0.27),\n",
      " ('vect3__sex', 0.26),\n",
      " ('vect3__ ass', 0.25),\n",
      " ('vect3__ dick', 0.25),\n",
      " ('vect3__dick', 0.25),\n",
      " ('vect3__ dic', 0.24),\n",
      " ('vect3__ sex', 0.24),\n",
      " ('vect3__ sh', 0.23),\n",
      " ('vect3__shit ', 0.23),\n",
      " ('vect3__ass', 0.21),\n",
      " ('vect3__ di', 0.19),\n",
      " ('vect3__ du', 0.18),\n",
      " ('vect3__!!!', 0.18),\n",
      " ('vect3__dic', 0.18),\n",
      " ('vect3__ex ', 0.18),\n",
      " ('vect3__hit ', 0.18)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "[('vect3__fuc', 0.58),\n",
      " ('vect3__fuck', 0.58),\n",
      " ('vect3__ fuc', 0.53),\n",
      " ('vect3__ fuck', 0.53),\n",
      " ('vect3__uck', 0.53),\n",
      " ('vect3__ fu', 0.41),\n",
      " ('vect3__shit', 0.38),\n",
      " ('vect3__hit', 0.36),\n",
      " ('vect3__ shit', 0.35),\n",
      " ('vect3__ck ', 0.35),\n",
      " ('vect3__fuck ', 0.35),\n",
      " ('vect3__uck ', 0.33),\n",
      " ('vect3__ shi', 0.29),\n",
      " ('vect3__shi', 0.28),\n",
      " ('vect3__ sh', 0.27),\n",
      " ('vect3__ ass ', 0.26),\n",
      " ('vect3__ dick', 0.26),\n",
      " ('vect3__sex', 0.26),\n",
      " ('vect3__dick', 0.25),\n",
      " ('vect3__ ass', 0.24),\n",
      " ('vect3__ dic', 0.24),\n",
      " ('vect3__ sex', 0.24),\n",
      " ('vect3__ick', 0.21),\n",
      " ('vect3__shit ', 0.21),\n",
      " ('vect3__ kil', 0.2),\n",
      " ('vect3__ kill', 0.2),\n",
      " ('vect3__kill', 0.2),\n",
      " ('vect3__ du', 0.19),\n",
      " ('vect3__kil', 0.19),\n",
      " ('vect3__ dum', 0.18)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "[('vect3__fuc', 0.59),\n",
      " ('vect3__fuck', 0.59),\n",
      " ('vect3__ fuc', 0.54),\n",
      " ('vect3__uck', 0.54),\n",
      " ('vect3__ fuck', 0.53),\n",
      " ('vect3__ fu', 0.42),\n",
      " ('vect3__ck ', 0.38),\n",
      " ('vect3__shit', 0.38),\n",
      " ('vect3__fuck ', 0.36),\n",
      " ('vect3__hit', 0.35),\n",
      " ('vect3__ shit', 0.34),\n",
      " ('vect3__uck ', 0.33),\n",
      " ('vect3__ ass', 0.29),\n",
      " ('vect3__ ass ', 0.29),\n",
      " ('vect3__ dick', 0.29),\n",
      " ('vect3__dick', 0.28),\n",
      " ('vect3__ shi', 0.27),\n",
      " ('vect3__shi', 0.27),\n",
      " ('vect3__ dic', 0.26),\n",
      " ('vect3__sex', 0.26),\n",
      " ('vect3__ sh', 0.24),\n",
      " ('vect3__ sex', 0.23),\n",
      " ('vect3__ass', 0.22),\n",
      " ('vect3__dic', 0.22),\n",
      " ('vect3__shit ', 0.22),\n",
      " ('vect3__ kil', 0.2),\n",
      " ('vect3__ kill', 0.2),\n",
      " ('vect3__ dum', 0.19),\n",
      " ('vect3__ dumb', 0.19),\n",
      " ('vect3__ ki', 0.19)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "[('vect3__fuc', 0.58),\n",
      " ('vect3__fuck', 0.57),\n",
      " ('vect3__uck', 0.55),\n",
      " ('vect3__ fuc', 0.51),\n",
      " ('vect3__ fuck', 0.51),\n",
      " ('vect3__ fu', 0.4),\n",
      " ('vect3__shit', 0.4),\n",
      " ('vect3__fuck ', 0.38),\n",
      " ('vect3__ck ', 0.36),\n",
      " ('vect3__hit', 0.36),\n",
      " ('vect3__ shit', 0.35),\n",
      " ('vect3__uck ', 0.35),\n",
      " ('vect3__ shi', 0.29),\n",
      " ('vect3__shi', 0.29),\n",
      " ('vect3__ ass ', 0.28),\n",
      " ('vect3__sex', 0.27),\n",
      " ('vect3__ dick', 0.26),\n",
      " ('vect3__ dic', 0.25),\n",
      " ('vect3__ sh', 0.25),\n",
      " ('vect3__dick', 0.25),\n",
      " ('vect3__ ass', 0.24),\n",
      " ('vect3__ sex', 0.24),\n",
      " ('vect3__shit ', 0.23),\n",
      " ('vect3__ass', 0.21),\n",
      " ('vect3__ as', 0.2),\n",
      " ('vect3__ di', 0.2),\n",
      " ('vect3__ du', 0.19),\n",
      " ('vect3__ rap', 0.19),\n",
      " ('vect3__dick ', 0.19),\n",
      " ('vect3__dum', 0.19)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "[('vect3__fuc', 0.59),\n",
      " ('vect3__fuck', 0.59),\n",
      " ('vect3__ fuc', 0.54),\n",
      " ('vect3__ fuck', 0.54),\n",
      " ('vect3__uck', 0.53),\n",
      " ('vect3__ fu', 0.43),\n",
      " ('vect3__fuck ', 0.38),\n",
      " ('vect3__shit', 0.38),\n",
      " ('vect3__hit', 0.35),\n",
      " ('vect3__ shit', 0.34),\n",
      " ('vect3__ck ', 0.34),\n",
      " ('vect3__uck ', 0.34),\n",
      " ('vect3__ ass ', 0.31),\n",
      " ('vect3__ ass', 0.28),\n",
      " ('vect3__ shi', 0.28),\n",
      " ('vect3__shi', 0.27),\n",
      " ('vect3__sex', 0.26),\n",
      " ('vect3__ dick', 0.25),\n",
      " ('vect3__ sh', 0.25),\n",
      " ('vect3__dick', 0.25),\n",
      " ('vect3__ dic', 0.23),\n",
      " ('vect3__ sex', 0.23),\n",
      " ('vect3__ass', 0.23),\n",
      " ('vect3__shit ', 0.23),\n",
      " ('vect3__ ki', 0.2),\n",
      " ('vect3__ kil', 0.2),\n",
      " ('vect3__ kill', 0.2),\n",
      " ('vect3__kill', 0.2),\n",
      " ('vect3__ as', 0.19),\n",
      " ('vect3__ rap', 0.19)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "[('vect3__fuc', 0.61),\n",
      " ('vect3__fuck', 0.61),\n",
      " ('vect3__ fuc', 0.56),\n",
      " ('vect3__uck', 0.56),\n",
      " ('vect3__ fuck', 0.55),\n",
      " ('vect3__ fu', 0.43),\n",
      " ('vect3__fuck ', 0.39),\n",
      " ('vect3__ck ', 0.37),\n",
      " ('vect3__shit', 0.37),\n",
      " ('vect3__ shit', 0.35),\n",
      " ('vect3__hit', 0.34),\n",
      " ('vect3__uck ', 0.34),\n",
      " ('vect3__ dick', 0.3),\n",
      " ('vect3__ dic', 0.29),\n",
      " ('vect3__dick', 0.29),\n",
      " ('vect3__ shi', 0.28),\n",
      " ('vect3__shi', 0.27),\n",
      " ('vect3__ ass', 0.26),\n",
      " ('vect3__ ass ', 0.26),\n",
      " ('vect3__sex', 0.26),\n",
      " ('vect3__ di', 0.25),\n",
      " ('vect3__shit ', 0.25),\n",
      " ('vect3__ sex', 0.23),\n",
      " ('vect3__ kill', 0.22),\n",
      " ('vect3__ sh', 0.22),\n",
      " ('vect3__ ki', 0.21),\n",
      " ('vect3__ kil', 0.21),\n",
      " ('vect3__ass', 0.21),\n",
      " ('vect3__hit ', 0.2),\n",
      " ('vect3__kill', 0.2)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "[('vect3__fuc', 0.57),\n",
      " ('vect3__fuck', 0.57),\n",
      " ('vect3__uck', 0.56),\n",
      " ('vect3__ fuc', 0.52),\n",
      " ('vect3__ fuck', 0.51),\n",
      " ('vect3__ fu', 0.4),\n",
      " ('vect3__shit', 0.39),\n",
      " ('vect3__ck ', 0.38),\n",
      " ('vect3__hit', 0.36),\n",
      " ('vect3__fuck ', 0.35),\n",
      " ('vect3__uck ', 0.35),\n",
      " ('vect3__ shit', 0.33),\n",
      " ('vect3__ ass', 0.28),\n",
      " ('vect3__ ass ', 0.28),\n",
      " ('vect3__ dic', 0.28),\n",
      " ('vect3__ dick', 0.28),\n",
      " ('vect3__shi', 0.28),\n",
      " ('vect3__dick', 0.27),\n",
      " ('vect3__sex', 0.27),\n",
      " ('vect3__ sex', 0.26),\n",
      " ('vect3__ shi', 0.25),\n",
      " ('vect3__ sh', 0.22),\n",
      " ('vect3__ass', 0.22),\n",
      " ('vect3__shit ', 0.22),\n",
      " ('vect3__dic', 0.21),\n",
      " ('vect3__ick', 0.21),\n",
      " ('vect3__ di', 0.2),\n",
      " ('vect3__ kil', 0.19),\n",
      " ('vect3__ kill', 0.19),\n",
      " ('vect3__ sex ', 0.19)]\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n"
     ]
    }
   ],
   "source": [
    "val_preds_arr1_ = np.zeros((df_val.shape[0], n_folds))\n",
    "val_preds_arr2_ = np.zeros((df_val.shape[0], n_folds))\n",
    "test_preds_arr_ = np.zeros((df_sub.shape[0], n_folds))\n",
    "\n",
    "for fld in range(n_folds):\n",
    "    pipeline = load('/kaggle/input/rudditridgeregression/withruddit'+str(fld)+'.joblib')\n",
    "    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n",
    "                                  np.round(pipeline['clf'].coef_,2) )), \n",
    "                         key = lambda x:x[1], \n",
    "                         reverse=True)\n",
    "\n",
    "    pprint(feature_wts[:30])\n",
    "    dump(pipeline,'/kaggle/working/withruddit'+str(fld)+'.joblib')\n",
    "    print(\"\\npredict validation data \")\n",
    "    val_preds_arr1_[:,fld] = pipeline.predict(df_val['less_toxic'])\n",
    "    val_preds_arr2_[:,fld] = pipeline.predict(df_val['more_toxic'])\n",
    "\n",
    "    print(\"\\npredict test data \")\n",
    "    test_preds_arr_[:,fld] = pipeline.predict(df_sub['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6733d737",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:36:53.454400Z",
     "iopub.status.busy": "2021-12-26T11:36:53.452910Z",
     "iopub.status.idle": "2021-12-26T11:36:53.462405Z",
     "shell.execute_reply": "2021-12-26T11:36:53.461992Z",
     "shell.execute_reply.started": "2021-12-25T10:27:00.295232Z"
    },
    "papermill": {
     "duration": 0.047837,
     "end_time": "2021-12-26T11:36:53.462508",
     "exception": false,
     "start_time": "2021-12-26T11:36:53.414671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Toxic data \n",
      "Validation Accuracy is 67.47\n",
      " Ruddit data \n",
      "Validation Accuracy is 62.57\n",
      " Toxic CLEAN data \n",
      "Validation Accuracy is 68.17\n"
     ]
    }
   ],
   "source": [
    "print(\" Toxic data \")\n",
    "p1 = val_preds_arr1.mean(axis=1)\n",
    "p2 = val_preds_arr2.mean(axis=1)\n",
    "\n",
    "print(f'Validation Accuracy is { np.round((p1 < p2).mean() * 100,2)}')\n",
    "\n",
    "print(\" Ruddit data \")\n",
    "p3 = val_preds_arr1_.mean(axis=1)\n",
    "p4 = val_preds_arr2_.mean(axis=1)\n",
    "\n",
    "print(f'Validation Accuracy is { np.round((p3 < p4).mean() * 100,2)}')\n",
    "\n",
    "print(\" Toxic CLEAN data \")\n",
    "p5 = val_preds_arr1c.mean(axis=1)\n",
    "p6 = val_preds_arr2c.mean(axis=1)\n",
    "\n",
    "print(f'Validation Accuracy is { np.round((p5 < p6).mean() * 100,2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a77683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:36:53.536758Z",
     "iopub.status.busy": "2021-12-26T11:36:53.536212Z",
     "iopub.status.idle": "2021-12-26T11:36:53.725982Z",
     "shell.execute_reply": "2021-12-26T11:36:53.726471Z",
     "shell.execute_reply.started": "2021-12-25T10:27:00.315165Z"
    },
    "papermill": {
     "duration": 0.228731,
     "end_time": "2021-12-26T11:36:53.726640",
     "exception": false,
     "start_time": "2021-12-26T11:36:53.497909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find right weight linear\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.69, 0.12, 0.19000000000000006, 67.93),\n",
       " (0.68, 0.13, 0.18999999999999995, 67.92),\n",
       " (0.69, 0.13, 0.18000000000000005, 67.92),\n",
       " (0.65, 0.16, 0.18999999999999997, 67.91),\n",
       " (0.67, 0.14, 0.18999999999999995, 67.91)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Find right weight linear\")\n",
    "\n",
    "wts_acc = []\n",
    "for i in range(30,70,1):\n",
    "    for j in range(0,20,1):\n",
    "        w1 = i/100\n",
    "        w2 = (100 - i - j)/100\n",
    "        w3 = (1 - w1 - w2 )\n",
    "        p1_wt = w1*p1 + w2*p3 + w3*p5\n",
    "        p2_wt = w1*p2 + w2*p4 + w3*p6\n",
    "        wts_acc.append( (w1,w2,w3, \n",
    "                         np.round((p1_wt < p2_wt).mean() * 100,2))\n",
    "                      )\n",
    "sorted(wts_acc, key=lambda x:x[3], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "869cb370",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:36:53.799080Z",
     "iopub.status.busy": "2021-12-26T11:36:53.798446Z",
     "iopub.status.idle": "2021-12-26T11:36:53.800901Z",
     "shell.execute_reply": "2021-12-26T11:36:53.801368Z",
     "shell.execute_reply.started": "2021-12-25T10:27:00.512771Z"
    },
    "papermill": {
     "duration": 0.04151,
     "end_time": "2021-12-26T11:36:53.801485",
     "exception": false,
     "start_time": "2021-12-26T11:36:53.759975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15562062, 0.07953136, 0.13364651, ..., 0.20036902, 0.20036902,\n",
       "       0.20036902])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1#p1代表less_toxic中的预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "863b5e64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:36:53.872297Z",
     "iopub.status.busy": "2021-12-26T11:36:53.871477Z",
     "iopub.status.idle": "2021-12-26T11:36:53.877299Z",
     "shell.execute_reply": "2021-12-26T11:36:53.876880Z",
     "shell.execute_reply.started": "2021-12-25T10:27:00.5227Z"
    },
    "papermill": {
     "duration": 0.041368,
     "end_time": "2021-12-26T11:36:53.877399",
     "exception": false,
     "start_time": "2021-12-26T11:36:53.836031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "w1,w2,w3,_ = sorted(wts_acc, key=lambda x:x[3], reverse=True)[0]\n",
    "#print(best_wts)\n",
    "\n",
    "p1_wt = w1*p1 + w2*p3 + w3*p5\n",
    "p2_wt = w1*p2 + w2*p4 + w3*p6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcc63904",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:36:53.950109Z",
     "iopub.status.busy": "2021-12-26T11:36:53.949396Z",
     "iopub.status.idle": "2021-12-26T11:36:53.952057Z",
     "shell.execute_reply": "2021-12-26T11:36:53.952566Z",
     "shell.execute_reply.started": "2021-12-25T10:27:00.531314Z"
    },
    "papermill": {
     "duration": 0.042092,
     "end_time": "2021-12-26T11:36:53.952702",
     "exception": false,
     "start_time": "2021-12-26T11:36:53.910610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30108"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7901456b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:36:54.026130Z",
     "iopub.status.busy": "2021-12-26T11:36:54.025367Z",
     "iopub.status.idle": "2021-12-26T11:36:54.052680Z",
     "shell.execute_reply": "2021-12-26T11:36:54.053180Z",
     "shell.execute_reply.started": "2021-12-25T10:27:00.540003Z"
    },
    "papermill": {
     "duration": 0.066207,
     "end_time": "2021-12-26T11:36:54.053322",
     "exception": false,
     "start_time": "2021-12-26T11:36:53.987115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>diff</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25062</th>\n",
       "      <td>246</td>\n",
       "      <td>CaliforniaAliBaba is a Bitch\\nGo fuck yourself you bitch ass motherfucker. Don't you ever in your fucking life threatening me you fucking piece of shit you. You don't know me I will fucking kill your ass you punk pussy bitch. tell your mama to my black dick.</td>\n",
       "      <td>I've heard a snippet on their myspace page but I'm afraid I haven't given them much though musically as of yet.  Perhaps I will be purchasing one of their albums since I seem to spend about a third of my time on wiki reverting their vandalism.</td>\n",
       "      <td>1.056383</td>\n",
       "      <td>0.094560</td>\n",
       "      <td>0.961823</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25754</th>\n",
       "      <td>190</td>\n",
       "      <td>what the hell man? why did you delete my page (savannah hoard) screw you dude\\n\\ni mean really? if you're not gonna let me have a page then you can go fuck your fat self you 40 year old bitch\\n\\nyour a total bitch who goes around deleting people's pages you fucking disgust me im gonna kick your ...</td>\n",
       "      <td>\" December 2009 (UTC)\\n\\n Now there's typical AGW alarmist Watermelon argumentum ad hominem content-vacant suppressive authoritarian WikiNazi rottenness if ever it got posted online. Don't address Monckton's (or Nsaa's) position, but strive (with utter ineffectiveness, I might add) to fault the ...</td>\n",
       "      <td>0.929921</td>\n",
       "      <td>0.169816</td>\n",
       "      <td>0.760106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28241</th>\n",
       "      <td>118</td>\n",
       "      <td>you are a stupid fuck \\n\\nand your mother's cunt stinks</td>\n",
       "      <td>\"\\n\\n  :::   According to Stratfordian authority H.N.Gibson(frequently cited in this very article) the controversy began with John Marston and Joseph Hall in 1597 which view was even more fully affirmed by Stratfordian authorities Garnett and Gosse over fifty years previously. According to Strat...</td>\n",
       "      <td>0.804608</td>\n",
       "      <td>0.098318</td>\n",
       "      <td>0.706290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>431</td>\n",
       "      <td>ASSHOLES! COME AND TRY ME YOU STUPID FUCK SON OF A BITCHES!!!!</td>\n",
       "      <td>\"\\n\\nPlease, Cheesebot is not nonsense. It is a formidible and tangible thing, can you please not delete this page again? I don't believe you are the authority in human experience in a position to deem other people's political and personal movements as \"\"nonsense.\"\" I appreciate. \"</td>\n",
       "      <td>0.868073</td>\n",
       "      <td>0.169549</td>\n",
       "      <td>0.698524</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19411</th>\n",
       "      <td>46</td>\n",
       "      <td>Ok, what is it \\n\\nWhy are you such a queer.. you have no power, so why the FUCK are you acting like you do. Grow up you fucking stupid shit head cock eating bastard.. seriously! Get a life, you're just a 17 yo piece of shit.</td>\n",
       "      <td>wow the guy who deleated the stuff on the Habbo NSM is leet. 88!</td>\n",
       "      <td>0.859645</td>\n",
       "      <td>0.179722</td>\n",
       "      <td>0.679924</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21059</th>\n",
       "      <td>333</td>\n",
       "      <td>fuck you \\n\\nyou're an idiot</td>\n",
       "      <td>What makes you an editor? Why do you use the name of a great dead man? What is your true name, profession, country? \\nShamir</td>\n",
       "      <td>0.888629</td>\n",
       "      <td>0.213125</td>\n",
       "      <td>0.675505</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29056</th>\n",
       "      <td>204</td>\n",
       "      <td>Fuck You\\n\\nGo suck on a fat dick faggot mother fucker. I'm the the wrong person to fuck with you puck ass pussy! I would keep my moth shut, or I can shut for you!</td>\n",
       "      <td>\"\\n\\nFUCCK U A$$HOLE do wateva u want  DIICK HEAD  Contact Blogsd ! \"</td>\n",
       "      <td>1.002026</td>\n",
       "      <td>0.340888</td>\n",
       "      <td>0.661138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29057</th>\n",
       "      <td>716</td>\n",
       "      <td>Fuck You\\n\\nGo suck on a fat dick faggot mother fucker. I'm the the wrong person to fuck with you puck ass pussy! I would keep my moth shut, or I can shut for you!</td>\n",
       "      <td>\"\\n\\nFUCCK U A$$HOLE do wateva u want  DIICK HEAD  Contact Blogsd ! \"</td>\n",
       "      <td>1.002026</td>\n",
       "      <td>0.340888</td>\n",
       "      <td>0.661138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10376</th>\n",
       "      <td>88</td>\n",
       "      <td>Yo Wuhwuzdat u a bitch wit no life. Yeah u heard me u a bitch. fuck u motherfucka</td>\n",
       "      <td>\"\\n\\n Name \\n\\nPaper Wrapped Cake?  That is lame.  That's not even its proper name!  How about sponge cake? — ¡ρρ¡ ¡ρρ¡ \"</td>\n",
       "      <td>0.735965</td>\n",
       "      <td>0.081028</td>\n",
       "      <td>0.654937</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22659</th>\n",
       "      <td>386</td>\n",
       "      <td>YOU SUCK \\n\\nI HATE YOU&gt; YOU CAN FUCK MY ASS ALL NIGHT BITCH SHit\\n\\nLove , David Ortiz</td>\n",
       "      <td>\"\\n\\n Antony and the Johnsons \\n\\nWell, Cardinal, is \"\"Antony's voice seems to channel Nina Simone and Bryan Ferry, and he has many celebrity admirers such as Philip Glass, Marc Almond, Lou Reed and the guest vocalists on I Am a Bird Now, Boy George, Rufus Wainwright and Devendra Banhart\"\" an op...</td>\n",
       "      <td>0.815075</td>\n",
       "      <td>0.163485</td>\n",
       "      <td>0.651590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7788</th>\n",
       "      <td>580</td>\n",
       "      <td>suck my dick \\n\\nfuck you dirty whore bitch</td>\n",
       "      <td>Stop being such a sissy.</td>\n",
       "      <td>0.917870</td>\n",
       "      <td>0.268828</td>\n",
       "      <td>0.649041</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20435</th>\n",
       "      <td>504</td>\n",
       "      <td>Fuck you niggerkite. y you deleting so much shit. fuckn loser. dont delete anymore shit. its none of your fucking business</td>\n",
       "      <td>Bush is the man. \\n\\nHe had the balls to dispose of Saddam and declare war on Islamic fundamentalism. Regardless of how he went about doing it, or his interest in oil, he did it. He can have the oil, if that's what he wants in return for ridding the world of religious tyranny (hopefully startin...</td>\n",
       "      <td>0.828406</td>\n",
       "      <td>0.190028</td>\n",
       "      <td>0.638379</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12472</th>\n",
       "      <td>639</td>\n",
       "      <td>Go fuck yourself you faggot -</td>\n",
       "      <td>\"::Natianal Socialism was just a political movement, like Republican. It only became a \"\"crime\"\" when the Germans lost the war. Imagine if US lost a war to someone, and being a Democrat made you a \"\"war criminal\"\". This is basically the situation thanks to the heebs.\\n\\n\"</td>\n",
       "      <td>0.837919</td>\n",
       "      <td>0.207251</td>\n",
       "      <td>0.630669</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12482</th>\n",
       "      <td>733</td>\n",
       "      <td>Steve Little \\n\\nSteve Little's character in Eastbound and Down is NOT BUMBLING. HE IS MILDLY RETARDED! Your ignorance in handling this matter angers me to a new level of hate for faggot wikipedia moderators who think they are fucking geniuses you guys are fucking retards who were born with no ...</td>\n",
       "      <td>The only thing I can considor I an attack was his edits. Which is acceptable as critism.</td>\n",
       "      <td>0.734228</td>\n",
       "      <td>0.109250</td>\n",
       "      <td>0.624978</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11226</th>\n",
       "      <td>320</td>\n",
       "      <td>I fucked your dad last night you fucking whore assed cunt.</td>\n",
       "      <td>Please don't try and mark a page for deletion our page is superior and you are simply lame. Please go away. Thanks...ur a tool.</td>\n",
       "      <td>0.769925</td>\n",
       "      <td>0.153309</td>\n",
       "      <td>0.616617</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13944</th>\n",
       "      <td>106</td>\n",
       "      <td>Hey \\n\\nhey faggot, are you dead yet? or are you watching your mom getting raped by filthy pigs? which one is it? I'm coming to rape your mom and shoot your entire family. You have 1 week to live, you motherfucker.</td>\n",
       "      <td>)\\n\\nNo images\\n\\n(vote here to remove all poo pix</td>\n",
       "      <td>0.743954</td>\n",
       "      <td>0.128391</td>\n",
       "      <td>0.615563</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17175</th>\n",
       "      <td>179</td>\n",
       "      <td>Gon Be A Snitch \\n\\nYou are a little bitch. I fuckin spent 6 hours on big sam (rapper) an you call it fuckin vandolism, hard ass aint you, fuck you i'm makin a true ass page an you wack off an delete my page. What the fuck is up, your like a snitch but to yourself. I'll fuckin edit your bitch a...</td>\n",
       "      <td>INDIAN ANUS IS STRETCHED AND WIDENED BY GRAWP’S MASSIVE COCK.</td>\n",
       "      <td>0.885747</td>\n",
       "      <td>0.273763</td>\n",
       "      <td>0.611983</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4865</th>\n",
       "      <td>528</td>\n",
       "      <td>Hey Man\\nYou are a retarded Mexican cocksucking piece of fucking shit. Please go fucking kill yourself. Wikipedia admins are lower then niggers! Fuck you!</td>\n",
       "      <td>you're a faggot for reporting handlfield</td>\n",
       "      <td>0.982929</td>\n",
       "      <td>0.375303</td>\n",
       "      <td>0.607626</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4866</th>\n",
       "      <td>609</td>\n",
       "      <td>Hey Man\\nYou are a retarded Mexican cocksucking piece of fucking shit. Please go fucking kill yourself. Wikipedia admins are lower then niggers! Fuck you!</td>\n",
       "      <td>you're a faggot for reporting handlfield</td>\n",
       "      <td>0.982929</td>\n",
       "      <td>0.375303</td>\n",
       "      <td>0.607626</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30096</th>\n",
       "      <td>146</td>\n",
       "      <td>eat shit an die faggot.</td>\n",
       "      <td>\" September 2011 (UTC)\\nOf course i read it! Your real question should be, if i care..... My talk page is free of speech and no one will indicate otherwise! F*ck anyone who don't agree &gt;;] THats how I am in real life, thats how i'll f*cking be on wikipedia! ;D  STACK STACK STACK \\nAnd PS, i didn...</td>\n",
       "      <td>0.785939</td>\n",
       "      <td>0.178933</td>\n",
       "      <td>0.607007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       worker  \\\n",
       "25062     246   \n",
       "25754     190   \n",
       "28241     118   \n",
       "1952      431   \n",
       "19411      46   \n",
       "21059     333   \n",
       "29056     204   \n",
       "29057     716   \n",
       "10376      88   \n",
       "22659     386   \n",
       "7788      580   \n",
       "20435     504   \n",
       "12472     639   \n",
       "12482     733   \n",
       "11226     320   \n",
       "13944     106   \n",
       "17175     179   \n",
       "4865      528   \n",
       "4866      609   \n",
       "30096     146   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                        less_toxic  \\\n",
       "25062                                           CaliforniaAliBaba is a Bitch\\nGo fuck yourself you bitch ass motherfucker. Don't you ever in your fucking life threatening me you fucking piece of shit you. You don't know me I will fucking kill your ass you punk pussy bitch. tell your mama to my black dick.   \n",
       "25754  what the hell man? why did you delete my page (savannah hoard) screw you dude\\n\\ni mean really? if you're not gonna let me have a page then you can go fuck your fat self you 40 year old bitch\\n\\nyour a total bitch who goes around deleting people's pages you fucking disgust me im gonna kick your ...   \n",
       "28241                                                                                                                                                                                                                                                      you are a stupid fuck \\n\\nand your mother's cunt stinks   \n",
       "1952                                                                                                                                                                                                                                                ASSHOLES! COME AND TRY ME YOU STUPID FUCK SON OF A BITCHES!!!!   \n",
       "19411                                                                            Ok, what is it \\n\\nWhy are you such a queer.. you have no power, so why the FUCK are you acting like you do. Grow up you fucking stupid shit head cock eating bastard.. seriously! Get a life, you're just a 17 yo piece of shit.   \n",
       "21059                                                                                                                                                                                                                                                                                 fuck you \\n\\nyou're an idiot   \n",
       "29056                                                                                                                                       Fuck You\\n\\nGo suck on a fat dick faggot mother fucker. I'm the the wrong person to fuck with you puck ass pussy! I would keep my moth shut, or I can shut for you!      \n",
       "29057                                                                                                                                       Fuck You\\n\\nGo suck on a fat dick faggot mother fucker. I'm the the wrong person to fuck with you puck ass pussy! I would keep my moth shut, or I can shut for you!      \n",
       "10376                                                                                                                                                                                                                            Yo Wuhwuzdat u a bitch wit no life. Yeah u heard me u a bitch. fuck u motherfucka   \n",
       "22659                                                                                                                                                                                                                      YOU SUCK \\n\\nI HATE YOU> YOU CAN FUCK MY ASS ALL NIGHT BITCH SHit\\n\\nLove , David Ortiz   \n",
       "7788                                                                                                                                                                                                                                                                   suck my dick \\n\\nfuck you dirty whore bitch   \n",
       "20435                                                                                                                                                                                   Fuck you niggerkite. y you deleting so much shit. fuckn loser. dont delete anymore shit. its none of your fucking business   \n",
       "12472                                                                                                                                                                                                                                                                              Go fuck yourself you faggot -     \n",
       "12482   Steve Little \\n\\nSteve Little's character in Eastbound and Down is NOT BUMBLING. HE IS MILDLY RETARDED! Your ignorance in handling this matter angers me to a new level of hate for faggot wikipedia moderators who think they are fucking geniuses you guys are fucking retards who were born with no ...   \n",
       "11226                                                                                                                                                                                                                                                   I fucked your dad last night you fucking whore assed cunt.   \n",
       "13944                                                                                       Hey \\n\\nhey faggot, are you dead yet? or are you watching your mom getting raped by filthy pigs? which one is it? I'm coming to rape your mom and shoot your entire family. You have 1 week to live, you motherfucker.   \n",
       "17175   Gon Be A Snitch \\n\\nYou are a little bitch. I fuckin spent 6 hours on big sam (rapper) an you call it fuckin vandolism, hard ass aint you, fuck you i'm makin a true ass page an you wack off an delete my page. What the fuck is up, your like a snitch but to yourself. I'll fuckin edit your bitch a...   \n",
       "4865                                                                                                                                                  Hey Man\\nYou are a retarded Mexican cocksucking piece of fucking shit. Please go fucking kill yourself. Wikipedia admins are lower then niggers! Fuck you!     \n",
       "4866                                                                                                                                                  Hey Man\\nYou are a retarded Mexican cocksucking piece of fucking shit. Please go fucking kill yourself. Wikipedia admins are lower then niggers! Fuck you!     \n",
       "30096                                                                                                                                                                                                                                                                                      eat shit an die faggot.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                        more_toxic  \\\n",
       "25062                                                       I've heard a snippet on their myspace page but I'm afraid I haven't given them much though musically as of yet.  Perhaps I will be purchasing one of their albums since I seem to spend about a third of my time on wiki reverting their vandalism.      \n",
       "25754  \" December 2009 (UTC)\\n\\n Now there's typical AGW alarmist Watermelon argumentum ad hominem content-vacant suppressive authoritarian WikiNazi rottenness if ever it got posted online. Don't address Monckton's (or Nsaa's) position, but strive (with utter ineffectiveness, I might add) to fault the ...   \n",
       "28241  \"\\n\\n  :::   According to Stratfordian authority H.N.Gibson(frequently cited in this very article) the controversy began with John Marston and Joseph Hall in 1597 which view was even more fully affirmed by Stratfordian authorities Garnett and Gosse over fifty years previously. According to Strat...   \n",
       "1952                    \"\\n\\nPlease, Cheesebot is not nonsense. It is a formidible and tangible thing, can you please not delete this page again? I don't believe you are the authority in human experience in a position to deem other people's political and personal movements as \"\"nonsense.\"\" I appreciate. \"   \n",
       "19411                                                                                                                                                                                                                                             wow the guy who deleated the stuff on the Habbo NSM is leet. 88!   \n",
       "21059                                                                                                                                                                                 What makes you an editor? Why do you use the name of a great dead man? What is your true name, profession, country? \\nShamir   \n",
       "29056                                                                                                                                                                                                                                        \"\\n\\nFUCCK U A$$HOLE do wateva u want  DIICK HEAD  Contact Blogsd ! \"   \n",
       "29057                                                                                                                                                                                                                                        \"\\n\\nFUCCK U A$$HOLE do wateva u want  DIICK HEAD  Contact Blogsd ! \"   \n",
       "10376                                                                                                                                                                                    \"\\n\\n Name \\n\\nPaper Wrapped Cake?  That is lame.  That's not even its proper name!  How about sponge cake? — ¡ρρ¡ ¡ρρ¡ \"   \n",
       "22659  \"\\n\\n Antony and the Johnsons \\n\\nWell, Cardinal, is \"\"Antony's voice seems to channel Nina Simone and Bryan Ferry, and he has many celebrity admirers such as Philip Glass, Marc Almond, Lou Reed and the guest vocalists on I Am a Bird Now, Boy George, Rufus Wainwright and Devendra Banhart\"\" an op...   \n",
       "7788                                                                                                                                                                                                                                                                                     Stop being such a sissy.    \n",
       "20435   Bush is the man. \\n\\nHe had the balls to dispose of Saddam and declare war on Islamic fundamentalism. Regardless of how he went about doing it, or his interest in oil, he did it. He can have the oil, if that's what he wants in return for ridding the world of religious tyranny (hopefully startin...   \n",
       "12472                             \"::Natianal Socialism was just a political movement, like Republican. It only became a \"\"crime\"\" when the Germans lost the war. Imagine if US lost a war to someone, and being a Democrat made you a \"\"war criminal\"\". This is basically the situation thanks to the heebs.\\n\\n\"   \n",
       "12482                                                                                                                                                                                                                     The only thing I can considor I an attack was his edits. Which is acceptable as critism.   \n",
       "11226                                                                                                                                                                              Please don't try and mark a page for deletion our page is superior and you are simply lame. Please go away. Thanks...ur a tool.   \n",
       "13944                                                                                                                                                                                                                                                           )\\n\\nNo images\\n\\n(vote here to remove all poo pix   \n",
       "17175                                                                                                                                                                                                                                                INDIAN ANUS IS STRETCHED AND WIDENED BY GRAWP’S MASSIVE COCK.   \n",
       "4865                                                                                                                                                                                                                                                                      you're a faggot for reporting handlfield   \n",
       "4866                                                                                                                                                                                                                                                                      you're a faggot for reporting handlfield   \n",
       "30096  \" September 2011 (UTC)\\nOf course i read it! Your real question should be, if i care..... My talk page is free of speech and no one will indicate otherwise! F*ck anyone who don't agree >;] THats how I am in real life, thats how i'll f*cking be on wikipedia! ;D  STACK STACK STACK \\nAnd PS, i didn...   \n",
       "\n",
       "             p1        p2      diff  correct  \n",
       "25062  1.056383  0.094560  0.961823        0  \n",
       "25754  0.929921  0.169816  0.760106        0  \n",
       "28241  0.804608  0.098318  0.706290        0  \n",
       "1952   0.868073  0.169549  0.698524        0  \n",
       "19411  0.859645  0.179722  0.679924        0  \n",
       "21059  0.888629  0.213125  0.675505        0  \n",
       "29056  1.002026  0.340888  0.661138        0  \n",
       "29057  1.002026  0.340888  0.661138        0  \n",
       "10376  0.735965  0.081028  0.654937        0  \n",
       "22659  0.815075  0.163485  0.651590        0  \n",
       "7788   0.917870  0.268828  0.649041        0  \n",
       "20435  0.828406  0.190028  0.638379        0  \n",
       "12472  0.837919  0.207251  0.630669        0  \n",
       "12482  0.734228  0.109250  0.624978        0  \n",
       "11226  0.769925  0.153309  0.616617        0  \n",
       "13944  0.743954  0.128391  0.615563        0  \n",
       "17175  0.885747  0.273763  0.611983        0  \n",
       "4865   0.982929  0.375303  0.607626        0  \n",
       "4866   0.982929  0.375303  0.607626        0  \n",
       "30096  0.785939  0.178933  0.607007        0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val['p1'] = p1_wt\n",
    "df_val['p2'] = p2_wt\n",
    "df_val['diff'] = np.abs(p2_wt - p1_wt)\n",
    "\n",
    "df_val['correct'] = (p1_wt < p2_wt).astype('int')\n",
    "### Incorrect predictions with dis-similar scores\n",
    "df_val[df_val.correct == 0].sort_values('diff', ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f393268",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:36:54.128812Z",
     "iopub.status.busy": "2021-12-26T11:36:54.128081Z",
     "iopub.status.idle": "2021-12-26T11:37:22.572692Z",
     "shell.execute_reply": "2021-12-26T11:37:22.573077Z",
     "shell.execute_reply.started": "2021-12-25T10:27:00.575475Z"
    },
    "papermill": {
     "duration": 28.483601,
     "end_time": "2021-12-26T11:37:22.573245",
     "exception": false,
     "start_time": "2021-12-26T11:36:54.089644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/pythonicforbertuse/pythonicforbert-1.0.12-py3-none-any.whl\r\n",
      "Requirement already satisfied: torch>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from pythonicforbert==1.0.12) (1.9.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.5.0->pythonicforbert==1.0.12) (3.10.0.2)\r\n",
      "Installing collected packages: pythonicforbert\r\n",
      "Successfully installed pythonicforbert-1.0.12\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ../input/pythonicforbertuse/pythonicforbert-1.0.12-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f78d3e91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:37:22.654359Z",
     "iopub.status.busy": "2021-12-26T11:37:22.653523Z",
     "iopub.status.idle": "2021-12-26T11:37:24.107052Z",
     "shell.execute_reply": "2021-12-26T11:37:24.106467Z",
     "shell.execute_reply.started": "2021-12-25T10:27:29.129876Z"
    },
    "papermill": {
     "duration": 1.497499,
     "end_time": "2021-12-26T11:37:24.107189",
     "exception": false,
     "start_time": "2021-12-26T11:37:22.609690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pythonicforbert\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "import torch.nn as nn\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        if mask_ids is not None:\n",
    "            mask_ids = mask_ids[:,:,None].float()\n",
    "            output -= 1e-12*(1.0-mask_ids)\n",
    "        output = output[:,0]\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44ae6481",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:37:24.194844Z",
     "iopub.status.busy": "2021-12-26T11:37:24.187575Z",
     "iopub.status.idle": "2021-12-26T11:37:29.381601Z",
     "shell.execute_reply": "2021-12-26T11:37:29.381116Z",
     "shell.execute_reply.started": "2021-12-25T10:27:30.870593Z"
    },
    "papermill": {
     "duration": 5.237587,
     "end_time": "2021-12-26T11:37:29.381742",
     "exception": false,
     "start_time": "2021-12-26T11:37:24.144155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import RobertaTokenizer\n",
    "vocab_file = '/kaggle/input/roberta-base/vocab.json'\n",
    " \n",
    "merges_file = '/kaggle/input/roberta-base/merges.txt'\n",
    "tokenizer = RobertaTokenizer(vocab_file, merges_file)\n",
    "#tokenizer = RobertaTokenizer.from_pretrained('../input/roberta-base')\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd13187b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:37:29.459517Z",
     "iopub.status.busy": "2021-12-26T11:37:29.457924Z",
     "iopub.status.idle": "2021-12-26T11:37:29.460156Z",
     "shell.execute_reply": "2021-12-26T11:37:29.460594Z",
     "shell.execute_reply.started": "2021-12-25T10:27:36.326069Z"
    },
    "papermill": {
     "duration": 0.042724,
     "end_time": "2021-12-26T11:37:29.460806",
     "exception": false,
     "start_time": "2021-12-26T11:37:29.418082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "roberta = get_model_function('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68ade60",
   "metadata": {
    "papermill": {
     "duration": 0.03576,
     "end_time": "2021-12-26T11:37:29.532208",
     "exception": false,
     "start_time": "2021-12-26T11:37:29.496448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "roberta模型与线性回归模型融合，每个roberta模型的权重在8~14之间，合在一起之后除以3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a77c7b7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:37:29.610420Z",
     "iopub.status.busy": "2021-12-26T11:37:29.609686Z",
     "iopub.status.idle": "2021-12-26T11:38:40.571781Z",
     "shell.execute_reply": "2021-12-26T11:38:40.572305Z",
     "shell.execute_reply.started": "2021-12-25T10:27:36.333691Z"
    },
    "papermill": {
     "duration": 71.004348,
     "end_time": "2021-12-26T11:38:40.572457",
     "exception": false,
     "start_time": "2021-12-26T11:37:29.568109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30108/30108 [00:33<00:00, 897.30it/s]\n",
      "100%|██████████| 30108/30108 [00:29<00:00, 1010.22it/s]\n",
      "100%|██████████| 7537/7537 [00:07<00:00, 998.76it/s] \n"
     ]
    }
   ],
   "source": [
    "lesstoxic_valid_dataset = TestDataset(df_val['less_toxic'],maxlen=500)\n",
    "lesstoxic_valid_loader = DataLoader(lesstoxic_valid_dataset,batch_size=16)\n",
    "moretoxic_valid_dataset = TestDataset(df_val['more_toxic'],maxlen=500)\n",
    "moretoxic_valid_loader = DataLoader(moretoxic_valid_dataset,batch_size=16)\n",
    "test_dataset = TestDataset(df_sub['text'],maxlen=500)\n",
    "test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33836f7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:38:40.977274Z",
     "iopub.status.busy": "2021-12-26T11:38:40.976751Z",
     "iopub.status.idle": "2021-12-26T11:59:21.819996Z",
     "shell.execute_reply": "2021-12-26T11:59:21.819526Z",
     "shell.execute_reply.started": "2021-12-25T10:28:46.212651Z"
    },
    "papermill": {
     "duration": 1241.05004,
     "end_time": "2021-12-26T11:59:21.820118",
     "exception": false,
     "start_time": "2021-12-26T11:38:40.770078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1882/1882 [09:05<00:00,  3.45it/s]\n",
      "100%|██████████| 1882/1882 [09:04<00:00,  3.45it/s]\n",
      "100%|██████████| 472/472 [02:16<00:00,  3.46it/s]\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('/kaggle/input/bestpointroberta/best_score0.7274436090225563split0-0.843.pth')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "less_toxic1,more_toxic1 = [],[]\n",
    "result_toxic1 = []\n",
    "for batch_token in tqdm(lesstoxic_valid_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        less_toxic1 = less_toxic1+current_point.cpu().numpy().tolist()\n",
    "\n",
    "for batch_token in tqdm(moretoxic_valid_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        more_toxic1 = more_toxic1+current_point.cpu().numpy().tolist()\n",
    "        \n",
    "for batch_token in tqdm(test_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        result_toxic1 = result_toxic1+current_point.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef59ee02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T11:59:24.385582Z",
     "iopub.status.busy": "2021-12-26T11:59:24.384958Z",
     "iopub.status.idle": "2021-12-26T12:19:59.107305Z",
     "shell.execute_reply": "2021-12-26T12:19:59.107719Z",
     "shell.execute_reply.started": "2021-12-25T11:19:35.726912Z"
    },
    "papermill": {
     "duration": 1236.00591,
     "end_time": "2021-12-26T12:19:59.107888",
     "exception": false,
     "start_time": "2021-12-26T11:59:23.101978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1882/1882 [09:05<00:00,  3.45it/s]\n",
      "100%|██████████| 1882/1882 [09:05<00:00,  3.45it/s]\n",
      "100%|██████████| 472/472 [02:16<00:00,  3.46it/s]\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('/kaggle/input/bestpointroberta/best_score0.7244756628413138seed13-0.838.pth')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "less_toxic2,more_toxic2 = [],[]\n",
    "result_toxic2 = []\n",
    "for batch_token in tqdm(lesstoxic_valid_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        less_toxic2 = less_toxic2+current_point.cpu().numpy().tolist()\n",
    "        \n",
    "for batch_token in tqdm(moretoxic_valid_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        more_toxic2 = more_toxic2+current_point.cpu().numpy().tolist()\n",
    "        \n",
    "for batch_token in tqdm(test_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        result_toxic2 = result_toxic2+current_point.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fac22335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T12:20:04.254474Z",
     "iopub.status.busy": "2021-12-26T12:20:04.253752Z",
     "iopub.status.idle": "2021-12-26T12:40:44.854006Z",
     "shell.execute_reply": "2021-12-26T12:40:44.854448Z",
     "shell.execute_reply.started": "2021-12-25T11:39:54.336499Z"
    },
    "papermill": {
     "duration": 1243.024032,
     "end_time": "2021-12-26T12:40:44.854612",
     "exception": false,
     "start_time": "2021-12-26T12:20:01.830580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1882/1882 [09:07<00:00,  3.44it/s]\n",
      "100%|██████████| 1882/1882 [09:07<00:00,  3.44it/s]\n",
      "100%|██████████| 472/472 [02:17<00:00,  3.44it/s]\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('/kaggle/input/bestpointroberta/best_score0.7270478828650574seed220.833.pth')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "less_toxic3,more_toxic3 = [],[]\n",
    "result_toxic3 = []\n",
    "for batch_token in tqdm(lesstoxic_valid_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        less_toxic3 = less_toxic3+current_point.cpu().numpy().tolist()\n",
    "        \n",
    "for batch_token in tqdm(moretoxic_valid_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        more_toxic3 = more_toxic3+current_point.cpu().numpy().tolist()\n",
    "        \n",
    "for batch_token in tqdm(test_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        result_toxic3 = result_toxic3+current_point.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff94700c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T12:40:51.829662Z",
     "iopub.status.busy": "2021-12-26T12:40:51.828031Z",
     "iopub.status.idle": "2021-12-26T12:40:51.830231Z",
     "shell.execute_reply": "2021-12-26T12:40:51.830654Z",
     "shell.execute_reply.started": "2021-12-25T12:08:52.446374Z"
    },
    "papermill": {
     "duration": 3.475009,
     "end_time": "2021-12-26T12:40:51.830806",
     "exception": false,
     "start_time": "2021-12-26T12:40:48.355797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "less_toxic1 = np.array(less_toxic1)\n",
    "less_toxic2 = np.array(less_toxic2)\n",
    "less_toxic3 = np.array(less_toxic3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87489942",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T12:40:59.219215Z",
     "iopub.status.busy": "2021-12-26T12:40:59.218336Z",
     "iopub.status.idle": "2021-12-26T13:17:09.384896Z",
     "shell.execute_reply": "2021-12-26T13:17:09.385420Z",
     "shell.execute_reply.started": "2021-12-25T12:13:16.414266Z"
    },
    "papermill": {
     "duration": 2173.642509,
     "end_time": "2021-12-26T13:17:09.385592",
     "exception": false,
     "start_time": "2021-12-26T12:40:55.743083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find right weight roberta\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0, 1.9000000000000001, 69.63),\n",
       " (0.0, 0.0, 2.4000000000000004, 69.63),\n",
       " (0.0, 1.0000000000000011e-20, 1.9000000000000001, 69.63),\n",
       " (0.0, 1.0000000000000014e-25, 2.4000000000000004, 69.63),\n",
       " (0.0, 2.0000000000000023e-20, 1.9000000000000001, 69.63)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Find right weight roberta\")\n",
    "#w1,w2,w3,_ = sorted(wts_acc, key=lambda x:x[2], reverse=True)[0]\n",
    "\n",
    "#p1_wt = w1*p1 + w2*p3 + w3*p5\n",
    "#p2_wt = w1*p2 + w2*p4 + w3*p6\n",
    "#p1_wt,p2_wt为之前保存的最好的验证集合的权重\n",
    "wts_acc = []\n",
    "for i in range(0,50):\n",
    "    for j in range(0,50):\n",
    "        for k in range(0,50):\n",
    "        #右边边界应该为15\n",
    "            i = i*0.1\n",
    "            j = j*0.1\n",
    "            k = k*0.1\n",
    "            less_toxic_wt = np.array(less_toxic1).dot(i)+np.array(less_toxic2).dot(j)+np.array(less_toxic3).dot(k)\n",
    "            #less_toxic_wt = i*less_toxic1 + j*less_toxic2 + k*less_toxic3\n",
    "            less_toxic_wt = less_toxic_wt+p1_wt\n",
    "            more_toxic_wt = np.array(more_toxic1).dot(i)+np.array(more_toxic2).dot(j)+np.array(more_toxic3).dot(k)\n",
    "            more_toxic_wt = more_toxic_wt+p2_wt\n",
    "            wts_acc.append((i,j,k, \n",
    "                         np.round((less_toxic_wt < more_toxic_wt).mean() * 100,2))\n",
    "                      )\n",
    "sorted(wts_acc, key=lambda x:x[3], reverse=True)[:5]\n",
    "#新的结果不能使用w1,w2,w3，因为w1,w2,w3要作为最后的结果内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cafd2766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T13:17:16.899732Z",
     "iopub.status.busy": "2021-12-26T13:17:16.896331Z",
     "iopub.status.idle": "2021-12-26T13:17:16.916538Z",
     "shell.execute_reply": "2021-12-26T13:17:16.917011Z",
     "shell.execute_reply.started": "2021-12-25T12:13:46.893072Z"
    },
    "papermill": {
     "duration": 3.587357,
     "end_time": "2021-12-26T13:17:16.917159",
     "exception": false,
     "start_time": "2021-12-26T13:17:13.329802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>diff</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>313</td>\n",
       "      <td>This article sucks \\n\\nwoo woo wooooooo</td>\n",
       "      <td>WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!!!!!!!!??????????????????????????????????????????????????????!!!!!!=\\nWHER IS YOUR SEXY PIC GONE FROM YOUR MAIN PAGE???????? put it back</td>\n",
       "      <td>0.729710</td>\n",
       "      <td>1.034622</td>\n",
       "      <td>0.023155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188</td>\n",
       "      <td>\"And yes, people should recognize that but they usually don't. One of the first objections you hear directed at someone who says he favors free markets is invariably \"\"There's no such thing as a free market.\"\" Ridiculously trivial. \"</td>\n",
       "      <td>Daphne Guinness \\n\\nTop of the mornin' my favourite Alien Life Form. I'm sorry, I can't marry you because you are a poor commoner so, therefore, unimportant. Can I have my dresses back. I hope your fat Oirish arse hasn't stretched them.</td>\n",
       "      <td>0.228523</td>\n",
       "      <td>0.858688</td>\n",
       "      <td>0.101788</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>Western Media?\\n\\nYup, because every crime in the entire world is caused by the west.  Racists.</td>\n",
       "      <td>\"Atom you don't believe actual photos of masturbation is sexually explicit photos. I am sorry to say they are sexually explicit and so you said yourself that the disclaimer is used for that purpose. Also, Wikipedia itself is targeted for minors. In addition, you say that Wikipedia does not have ...</td>\n",
       "      <td>0.501397</td>\n",
       "      <td>0.794345</td>\n",
       "      <td>0.047356</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>347</td>\n",
       "      <td>And you removed it! You numbskull! I don't care what you say anymore, this is my life! Go ahead with your own life, leave me alone! —</td>\n",
       "      <td>You seem to have sand in your vagina.\\n\\nMight want to wash that stinking hole out, bitch.</td>\n",
       "      <td>0.827120</td>\n",
       "      <td>1.547908</td>\n",
       "      <td>0.157101</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>539</td>\n",
       "      <td>smelly vagina \\n\\nBluerasberry why don't you be a model for a cheesy blue vagina syndrome. A lot are experiencing this vaginal disease.</td>\n",
       "      <td>hey \\n\\nway to support nazis, you racist</td>\n",
       "      <td>0.991378</td>\n",
       "      <td>1.110849</td>\n",
       "      <td>0.056620</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   worker  \\\n",
       "0     313   \n",
       "1     188   \n",
       "2      82   \n",
       "3     347   \n",
       "4     539   \n",
       "\n",
       "                                                                                                                                                                                                                                  less_toxic  \\\n",
       "0                                                                                                                                                                                                    This article sucks \\n\\nwoo woo wooooooo   \n",
       "1  \"And yes, people should recognize that but they usually don't. One of the first objections you hear directed at someone who says he favors free markets is invariably \"\"There's no such thing as a free market.\"\" Ridiculously trivial. \"   \n",
       "2                                                                                                                                         Western Media?\\n\\nYup, because every crime in the entire world is caused by the west.  Racists.      \n",
       "3                                                                                                     And you removed it! You numbskull! I don't care what you say anymore, this is my life! Go ahead with your own life, leave me alone! —    \n",
       "4                                                                                                    smelly vagina \\n\\nBluerasberry why don't you be a model for a cheesy blue vagina syndrome. A lot are experiencing this vaginal disease.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    more_toxic  \\\n",
       "0                                                                                                                       WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!!!!!!!!??????????????????????????????????????????????????????!!!!!!=\\nWHER IS YOUR SEXY PIC GONE FROM YOUR MAIN PAGE???????? put it back   \n",
       "1                                                                 Daphne Guinness \\n\\nTop of the mornin' my favourite Alien Life Form. I'm sorry, I can't marry you because you are a poor commoner so, therefore, unimportant. Can I have my dresses back. I hope your fat Oirish arse hasn't stretched them.   \n",
       "2  \"Atom you don't believe actual photos of masturbation is sexually explicit photos. I am sorry to say they are sexually explicit and so you said yourself that the disclaimer is used for that purpose. Also, Wikipedia itself is targeted for minors. In addition, you say that Wikipedia does not have ...   \n",
       "3                                                                                                                                                                                                                   You seem to have sand in your vagina.\\n\\nMight want to wash that stinking hole out, bitch.   \n",
       "4                                                                                                                                                                                                                                                                     hey \\n\\nway to support nazis, you racist   \n",
       "\n",
       "         p1        p2      diff  correct  \n",
       "0  0.729710  1.034622  0.023155        1  \n",
       "1  0.228523  0.858688  0.101788        1  \n",
       "2  0.501397  0.794345  0.047356        1  \n",
       "3  0.827120  1.547908  0.157101        1  \n",
       "4  0.991378  1.110849  0.056620        1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#w1,w2,w3,_ = sorted(wts_acc, key=lambda x:x[2], reverse=True)[0]\n",
    "#p1_wt = w1*p1 + w2*p3 + w3*p5\n",
    "#p2_wt = w1*p2 + w2*p4 + w3*p6\n",
    "#前面计算出来的线性回归的w1,w2,w3是线性回归中最好的权重内容\n",
    "w4,w5,w6,_ = sorted(wts_acc, key=lambda x:x[3], reverse=True)[0]\n",
    "#易错点：这里的key=lambda一定为x:x[3]，去查看第三位的得分\n",
    "df_val['p1'] = less_toxic_wt+p1_wt\n",
    "df_val['p2'] = more_toxic_wt+p2_wt\n",
    "df_val['diff'] = np.abs(p2_wt - p1_wt)     \n",
    "\n",
    "df_val['correct'] = (p1_wt < p2_wt).astype('int')\n",
    "### Incorrect predictions with dis-similar scores\n",
    "df_val[df_val.correct == 0].sort_values('diff', ascending=False).head(20)\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebaf199a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T13:17:24.422796Z",
     "iopub.status.busy": "2021-12-26T13:17:24.421879Z",
     "iopub.status.idle": "2021-12-26T13:17:24.425107Z",
     "shell.execute_reply": "2021-12-26T13:17:24.425577Z",
     "shell.execute_reply.started": "2021-12-25T12:15:24.850373Z"
    },
    "papermill": {
     "duration": 3.9959,
     "end_time": "2021-12-26T13:17:24.425745",
     "exception": false,
     "start_time": "2021-12-26T13:17:20.429845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 = 0.690000,w2 = 0.120000,w3 = 0.190000\n",
      "\n",
      "w4 = 0,w5 = 0,w6 = 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('w1 = %f,w2 = %f,w3 = %f\\n'%(w1,w2,w3))\n",
    "print('w4 = %d,w5 = %d,w6 = %d\\n'%(w4,w5,w6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd3643c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T13:17:31.639623Z",
     "iopub.status.busy": "2021-12-26T13:17:31.638984Z",
     "iopub.status.idle": "2021-12-26T13:17:31.692367Z",
     "shell.execute_reply": "2021-12-26T13:17:31.691409Z",
     "shell.execute_reply.started": "2021-12-25T12:16:43.59447Z"
    },
    "papermill": {
     "duration": 3.733526,
     "end_time": "2021-12-26T13:17:31.692508",
     "exception": false,
     "start_time": "2021-12-26T13:17:27.958982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_submission = pd.read_csv('/kaggle/input/jigsaw-toxic-severity-rating/sample_submission.csv')\n",
    "df_submission['score'] = w1*test_preds_arr.mean(axis=1) + w2*test_preds_arr_.mean(axis=1) + w3*test_preds_arrc.mean(axis=1) + \\\n",
    "                         np.dot(np.array(result_toxic1),w4) + np.dot(np.array(result_toxic2),w5) + np.dot(np.array(result_toxic3),w6)\n",
    "#df_submission['score'] = np.dot(np.array(result_toxic1),w4) + np.dot(np.array(result_toxic2),w5) + np.dot(np.array(result_toxic3),w6)\n",
    "df_submission[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7090.287871,
   "end_time": "2021-12-26T13:17:39.229463",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-26T11:19:28.941592",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
