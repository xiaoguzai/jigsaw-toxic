{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c78e47ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re \n",
    "from scipy import sparse\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_colwidth=300\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c57de437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import emoji\n",
    "\n",
    "CUSTOM_TABLE = str.maketrans(\n",
    "    {\n",
    "        \"\\xad\": None,\n",
    "        \"\\x7f\": None,\n",
    "        \"\\ufeff\": None,\n",
    "        \"\\u200b\": None,\n",
    "        \"\\u200e\": None,\n",
    "        \"\\u202a\": None,\n",
    "        \"\\u202c\": None,\n",
    "        \"‘\": \"'\",\n",
    "        \"’\": \"'\",\n",
    "        \"`\": \"'\",\n",
    "        \"“\": '\"',\n",
    "        \"”\": '\"',\n",
    "        \"«\": '\"',\n",
    "        \"»\": '\"',\n",
    "        \"ɢ\": \"G\",\n",
    "        \"ɪ\": \"I\",\n",
    "        \"ɴ\": \"N\",\n",
    "        \"ʀ\": \"R\",\n",
    "        \"ʏ\": \"Y\",\n",
    "        \"ʙ\": \"B\",\n",
    "        \"ʜ\": \"H\",\n",
    "        \"ʟ\": \"L\",\n",
    "        \"ғ\": \"F\",\n",
    "        \"ᴀ\": \"A\",\n",
    "        \"ᴄ\": \"C\",\n",
    "        \"ᴅ\": \"D\",\n",
    "        \"ᴇ\": \"E\",\n",
    "        \"ᴊ\": \"J\",\n",
    "        \"ᴋ\": \"K\",\n",
    "        \"ᴍ\": \"M\",\n",
    "        \"Μ\": \"M\",\n",
    "        \"ᴏ\": \"O\",\n",
    "        \"ᴘ\": \"P\",\n",
    "        \"ᴛ\": \"T\",\n",
    "        \"ᴜ\": \"U\",\n",
    "        \"ᴡ\": \"W\",\n",
    "        \"ᴠ\": \"V\",\n",
    "        \"ĸ\": \"K\",\n",
    "        \"в\": \"B\",\n",
    "        \"м\": \"M\",\n",
    "        \"н\": \"H\",\n",
    "        \"т\": \"T\",\n",
    "        \"ѕ\": \"S\",\n",
    "        \"—\": \"-\",\n",
    "        \"–\": \"-\",\n",
    "    }\n",
    ")\n",
    "\n",
    "WORDS_REPLACER = [\n",
    "    (\"sh*t\", \"shit\"),\n",
    "    (\"s**t\", \"shit\"),\n",
    "    (\"f*ck\", \"fuck\"),\n",
    "    (\"fu*k\", \"fuck\"),\n",
    "    (\"f**k\", \"fuck\"),\n",
    "    (\"f*****g\", \"fucking\"),\n",
    "    (\"f***ing\", \"fucking\"),\n",
    "    (\"f**king\", \"fucking\"),\n",
    "    (\"p*ssy\", \"pussy\"),\n",
    "    (\"p***y\", \"pussy\"),\n",
    "    (\"pu**y\", \"pussy\"),\n",
    "    (\"p*ss\", \"piss\"),\n",
    "    (\"b*tch\", \"bitch\"),\n",
    "    (\"bit*h\", \"bitch\"),\n",
    "    (\"h*ll\", \"hell\"),\n",
    "    (\"h**l\", \"hell\"),\n",
    "    (\"cr*p\", \"crap\"),\n",
    "    (\"d*mn\", \"damn\"),\n",
    "    (\"stu*pid\", \"stupid\"),\n",
    "    (\"st*pid\", \"stupid\"),\n",
    "    (\"n*gger\", \"nigger\"),\n",
    "    (\"n***ga\", \"nigger\"),\n",
    "    (\"f*ggot\", \"faggot\"),\n",
    "    (\"scr*w\", \"screw\"),\n",
    "    (\"pr*ck\", \"prick\"),\n",
    "    (\"g*d\", \"god\"),\n",
    "    (\"s*x\", \"sex\"),\n",
    "    (\"a*s\", \"ass\"),\n",
    "    (\"a**hole\", \"asshole\"),\n",
    "    (\"a***ole\", \"asshole\"),\n",
    "    (\"a**\", \"ass\"),\n",
    "]\n",
    "\n",
    "REGEX_REPLACER = [\n",
    "    (re.compile(pat.replace(\"*\", \"\\*\"), flags=re.IGNORECASE), repl)\n",
    "    for pat, repl in WORDS_REPLACER\n",
    "]\n",
    "\n",
    "RE_SPACE = re.compile(r\"\\s\")\n",
    "RE_MULTI_SPACE = re.compile(r\"\\s+\")\n",
    "\n",
    "NMS_TABLE = dict.fromkeys(\n",
    "    i for i in range(sys.maxunicode + 1) if unicodedata.category(chr(i)) == \"Mn\"\n",
    ")\n",
    "\n",
    "HEBREW_TABLE = {i: \"א\" for i in range(0x0590, 0x05FF)}\n",
    "ARABIC_TABLE = {i: \"ا\" for i in range(0x0600, 0x06FF)}\n",
    "CHINESE_TABLE = {i: \"是\" for i in range(0x4E00, 0x9FFF)}\n",
    "KANJI_TABLE = {i: \"ッ\" for i in range(0x2E80, 0x2FD5)}\n",
    "HIRAGANA_TABLE = {i: \"ッ\" for i in range(0x3041, 0x3096)}\n",
    "KATAKANA_TABLE = {i: \"ッ\" for i in range(0x30A0, 0x30FF)}\n",
    "\n",
    "TABLE = dict()\n",
    "TABLE.update(CUSTOM_TABLE)\n",
    "TABLE.update(NMS_TABLE)\n",
    "# Non-english languages\n",
    "TABLE.update(CHINESE_TABLE)\n",
    "TABLE.update(HEBREW_TABLE)\n",
    "TABLE.update(ARABIC_TABLE)\n",
    "TABLE.update(HIRAGANA_TABLE)\n",
    "TABLE.update(KATAKANA_TABLE)\n",
    "TABLE.update(KANJI_TABLE)\n",
    "\n",
    "\n",
    "EMOJI_REGEXP = emoji.get_emoji_regexp()\n",
    "\n",
    "UNICODE_EMOJI_MY = {\n",
    "    k: f\" EMJ {v.strip(':').replace('_', ' ')} \"\n",
    "    for k, v in emoji.UNICODE_EMOJI_ALIAS_ENGLISH.items()\n",
    "}\n",
    "\n",
    "\n",
    "def my_demojize(string: str) -> str:\n",
    "    \"\"\"\n",
    "    Modified version of the demojize function from emoji\n",
    "    \"\"\"\n",
    "\n",
    "    def replace(match):\n",
    "        return UNICODE_EMOJI_MY.get(match.group(0), match.group(0))\n",
    "\n",
    "    return re.sub(\"\\ufe0f\", \"\", EMOJI_REGEXP.sub(replace, string))\n",
    "\n",
    "\n",
    "def fix_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Expects a list of lower-cased tokens from TweeterTokenizer\n",
    "    \"\"\"\n",
    "    for token in tokens:\n",
    "        if token == \"gov't\" or token == \"govt\":\n",
    "            yield \"government\"\n",
    "        elif token == \"i'm\":\n",
    "            yield \"i\"\n",
    "            yield \"am\"\n",
    "        elif token.endswith(\"n't\"):\n",
    "            yield token[:-3]\n",
    "            yield \"not\"\n",
    "        elif token.endswith(\"'re\"):\n",
    "            yield token[:-3]\n",
    "            yield \"are\"\n",
    "        elif token.endswith(\"'ll\"):\n",
    "            yield token[:-3]\n",
    "            yield \"will\"\n",
    "        elif token.endswith(\"'ve\"):\n",
    "            yield token[:-3]\n",
    "            yield \"have\"\n",
    "        elif token.endswith(\"'s\"):\n",
    "            yield token[:-2]\n",
    "            yield \"'s\"\n",
    "        else:\n",
    "            yield token\n",
    "\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = my_demojize(text)\n",
    "    text = RE_SPACE.sub(\" \", text)\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = text.translate(TABLE)\n",
    "    text = RE_MULTI_SPACE.sub(\" \", text).strip()\n",
    "\n",
    "    for pattern, repl in REGEX_REPLACER:\n",
    "        text = pattern.sub(repl, text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6298af84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>txt</th>\n",
       "      <th>url</th>\n",
       "      <th>offensiveness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza1q49</td>\n",
       "      <td>&gt; The difference in average earnings between men and women can be explained by taking into account relevant factors.\\n\\nSo it isn't a myth, you just feel that you can explain it.</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments/42g75o/cmv_the_so_called_wage_gap_is_a_myth//cza1q49/</td>\n",
       "      <td>-0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza1wdh</td>\n",
       "      <td>The myth is that the \"gap\" is entirely based on the sex of the  person.</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments/42g75o/cmv_the_so_called_wage_gap_is_a_myth//cza1wdh/</td>\n",
       "      <td>-0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza23qx</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments/42g75o/cmv_the_so_called_wage_gap_is_a_myth//cza23qx/</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza2bw8</td>\n",
       "      <td>The assertion is that women get paid less for the *same* jobs, and that they get paid less *because* they are women.</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments/42g75o/cmv_the_so_called_wage_gap_is_a_myth//cza2bw8/</td>\n",
       "      <td>-0.146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42g75o</td>\n",
       "      <td>cza2iji</td>\n",
       "      <td>You said in the OP that's not what they're measuring. They're measuring aggregate. \\n\\nAs entertainers, women make less. Isn't that what JLaw was complaining about?</td>\n",
       "      <td>https://www.reddit.com/r/changemyview/comments/42g75o/cmv_the_so_called_wage_gap_is_a_myth//cza2iji/</td>\n",
       "      <td>-0.083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id comment_id  \\\n",
       "0  42g75o    cza1q49   \n",
       "1  42g75o    cza1wdh   \n",
       "2  42g75o    cza23qx   \n",
       "3  42g75o    cza2bw8   \n",
       "4  42g75o    cza2iji   \n",
       "\n",
       "                                                                                                                                                                                  txt  \\\n",
       "0  > The difference in average earnings between men and women can be explained by taking into account relevant factors.\\n\\nSo it isn't a myth, you just feel that you can explain it.   \n",
       "1                                                                                                            The myth is that the \"gap\" is entirely based on the sex of the  person.    \n",
       "2                                                                                                                                                                           [deleted]   \n",
       "3                                                               The assertion is that women get paid less for the *same* jobs, and that they get paid less *because* they are women.    \n",
       "4                You said in the OP that's not what they're measuring. They're measuring aggregate. \\n\\nAs entertainers, women make less. Isn't that what JLaw was complaining about?   \n",
       "\n",
       "                                                                                                    url  \\\n",
       "0  https://www.reddit.com/r/changemyview/comments/42g75o/cmv_the_so_called_wage_gap_is_a_myth//cza1q49/   \n",
       "1  https://www.reddit.com/r/changemyview/comments/42g75o/cmv_the_so_called_wage_gap_is_a_myth//cza1wdh/   \n",
       "2  https://www.reddit.com/r/changemyview/comments/42g75o/cmv_the_so_called_wage_gap_is_a_myth//cza23qx/   \n",
       "3  https://www.reddit.com/r/changemyview/comments/42g75o/cmv_the_so_called_wage_gap_is_a_myth//cza2bw8/   \n",
       "4  https://www.reddit.com/r/changemyview/comments/42g75o/cmv_the_so_called_wage_gap_is_a_myth//cza2iji/   \n",
       "\n",
       "   offensiveness_score  \n",
       "0               -0.083  \n",
       "1               -0.022  \n",
       "2                0.167  \n",
       "3               -0.146  \n",
       "4               -0.083  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv('/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit/ruddit_with_text.csv')\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d1bad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'] = df['offensiveness_score']\n",
    "df['text'] = df['txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8faea3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char_wb', max_df=0.5, max_features=46000, min_df=3,\n",
       "                ngram_range=(3, 5))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_cleaning(text):\n",
    "    '''\n",
    "    Cleans text into a basic form for NLP. Operations include the following:-\n",
    "    1. Remove special charecters like &, #, etc\n",
    "    2. Removes extra spaces\n",
    "    3. Removes embedded URL links\n",
    "    4. Removes HTML tags\n",
    "    5. Removes emojis\n",
    "    \n",
    "    text - Text piece to be cleaned.\n",
    "    '''\n",
    "    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n",
    "    text = template.sub(r'', text)\n",
    "    \n",
    "    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n",
    "    only_text = soup.get_text()\n",
    "    text = only_text\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n",
    "    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n",
    "    text = text.strip() # remove spaces at the beginning and at the end of string\n",
    "\n",
    "\n",
    "    # text = text.lower()\n",
    "    # Clean some punctutations\n",
    "    text = text.replace('\\n', ' \\n ')\n",
    "    # Remove all numbers\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    # Add space between pattern\n",
    "    text = re.sub(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3', text)\n",
    "    # Replace repeating puncts more than 3 times to length of 3\n",
    "    text = re.sub(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1', text)    \n",
    "    # Add space around repeating puncts\n",
    "    text = re.sub(r'([*!?\\']+)',r' \\1 ', text)    \n",
    "    # patterns with repeating characters \n",
    "    # naaaahhhhhhh to naaaahh\n",
    "    text = re.sub(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1', text)\n",
    "    # naaaahh to naaahh\n",
    "    text = re.sub(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1', text)\n",
    "    # multiple spaces to one\n",
    "    text = re.sub(r'[ ]{2,}',' ', text).strip()   \n",
    "    text = normalize(text)\n",
    "\n",
    "\n",
    "    return text\n",
    "\n",
    "df = df.dropna(axis = 0)\n",
    "\n",
    "vec = TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5), max_features = 46000)\n",
    "vec.fit(df['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d87e5869",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/xiaoguzai/模型/unbiased-toxic-roberta/Jigsaw-Fasttext-Word-Embeddings-256D.bin.wv.vectors_ngrams.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22123/3183301380.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/xiaoguzai/模型/unbiased-toxic-roberta/Jigsaw-Fasttext-Word-Embeddings-256D.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \"\"\"\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFastText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrethrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1928\u001b[0m         \"\"\"\n\u001b[1;32m   1929\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1930\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1931\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1932\u001b[0m                 \u001b[0mrethrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_lifecycle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36m_load_specials\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;34m\"\"\"Handle special requirements of `.load()` protocol, usually up-converting older versions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFastText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bucket'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;31m# should only exist in one place: the wv subcomponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_load_specials\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1944\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1945\u001b[0m         \u001b[0;34m\"\"\"Handle special requirements of `.load()` protocol, usually up-converting older versions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1946\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1947\u001b[0m         \u001b[0;31m# for backward compatibility, add/rearrange properties from prior versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1948\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m_load_specials\u001b[0;34m(self, fname, mmap, compress, subname)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading %s recursively from %s.* with mmap=%s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mignore_deprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattrib\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__numpys'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36m_load_specials\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;34m\"\"\"Handle special requirements of `.load()` protocol, usually up-converting older versions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFastTextKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastTextKeyedVectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded object of type %s, not expected FastTextKeyedVectors\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_specials\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;34m\"\"\"Handle special requirements of `.load()` protocol, usually up-converting older versions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'doctags'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_upconvert_old_d2vkv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m_load_specials\u001b[0;34m(self, fname, mmap, compress, subname)\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mignore_deprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/xiaoguzai/模型/unbiased-toxic-roberta/Jigsaw-Fasttext-Word-Embeddings-256D.bin.wv.vectors_ngrams.npy'"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors, FastText\n",
    "\n",
    "fmodel = FastText.load('/home/xiaoguzai/模型/unbiased-toxic-roberta/Jigsaw-Fasttext-Word-Embeddings-256D.bin')\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "def splitter(text):\n",
    "    tokens = []\n",
    "    \n",
    "    for word in text.split(' '):\n",
    "        tokens.append(word)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def vectorizer(text):\n",
    "    tokens = splitter(text)\n",
    "    \n",
    "    x1 = vec.transform([text]).toarray()\n",
    "    x2 = np.mean(fmodel.wv[tokens], axis = 0).reshape(1, -1)\n",
    "    x = np.concatenate([x1, x2], axis = -1).astype(np.float16)\n",
    "    del x1\n",
    "    del x2 \n",
    "    \n",
    "    return x   \n",
    "\n",
    "X_list = []\n",
    "\n",
    "for text in df.text:\n",
    "    X_list.append(vectorizer(text))\n",
    "\n",
    "EMB_DIM = len(vec.vocabulary_) + 256\n",
    "\n",
    "X_np = np.array(X_list).reshape(-1, EMB_DIM)\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "X = sparse.csr_matrix(X_np)\n",
    "del X_np\n",
    "\n",
    "\n",
    "model = Ridge(alpha=0.5)\n",
    "model.fit(X, df['y'])\n",
    "\n",
    "\n",
    "l_model = Ridge(alpha=1.)\n",
    "l_model.fit(X, df['y'])\n",
    "\n",
    "\n",
    "d_model = Ridge(alpha=1.5)\n",
    "d_model.fit(X, df['y'])\n",
    "\n",
    "\n",
    "s_model = Ridge(alpha=2.)\n",
    "s_model.fit(X, df['y'])\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('model.pkl','wb') as f:\n",
    "    pickle.dump(model,f) \n",
    "with open('lmodel.pkl','wb') as f:\n",
    "    pickle.dump(l_model,f)\n",
    "with open('dmodel.pkl','wb') as f:\n",
    "    pickle.dump(d_model,f)\n",
    "with open('smodel.pkl','wb') as f:\n",
    "    pickle.dump(s_model,f)\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# with open('../input/fasttext-ridge/model.pkl','rb') as f:\n",
    "#     model = pickle.load(f)\n",
    "# with open('../input/fasttext-ridge/lmodel.pkl','rb') as f:\n",
    "#     l_model = pickle.load(f)\n",
    "# with open('../input/fasttext-ridge/dmodel.pkl','rb') as f:\n",
    "#     d_model = pickle.load(f)\n",
    "# with open('../input/fasttext-ridge/smodel.pkl','rb') as f:\n",
    "#     s_model = pickle.load(f)\n",
    "\n",
    "df_val = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "# df_val = pd.read_csv(\"/content/drive/Shareddrives/Jigsaw/validation_true.csv\")\n",
    "\n",
    "tqdm.pandas()\n",
    "df_val['less_toxic'] = df_val['less_toxic'].progress_apply(text_cleaning)\n",
    "df_val['more_toxic'] = df_val['more_toxic'].progress_apply(text_cleaning)\n",
    "\n",
    "\n",
    "df_val.to_csv(\"cleaned_valid4.csv\")\n",
    "\n",
    "X_less_toxic_temp = []\n",
    "\n",
    "for text in df_val.less_toxic:\n",
    "    X_less_toxic_temp.append(vectorizer(text))\n",
    "\n",
    "X_less_toxic_temp = np.array(X_less_toxic_temp).reshape(-1, EMB_DIM)\n",
    "X_less_toxic = sparse.csr_matrix(X_less_toxic_temp)\n",
    "\n",
    "del X_less_toxic_temp\n",
    "    \n",
    "X_more_toxic_temp = []\n",
    "\n",
    "for text in df_val.more_toxic:\n",
    "    X_more_toxic_temp.append(vectorizer(text))\n",
    "    \n",
    "X_more_toxic_temp = np.array(X_more_toxic_temp).reshape(-1, EMB_DIM)\n",
    "X_more_toxic = sparse.csr_matrix(X_more_toxic_temp)\n",
    "\n",
    "del X_more_toxic_temp\n",
    "\n",
    "less1 = model.predict(X_less_toxic)\n",
    "more1 = model.predict(X_more_toxic)\n",
    "# Validation Accuracy\n",
    "print((less1 < more1).mean())\n",
    "\n",
    "\n",
    "less2  = l_model.predict(X_less_toxic)\n",
    "more2 = l_model.predict(X_more_toxic)\n",
    "# Validation Accuracy\n",
    "print((less2 < more2).mean())\n",
    "\n",
    "\n",
    "less3 = d_model.predict(X_less_toxic)\n",
    "more3 = d_model.predict(X_more_toxic)\n",
    "# Validation Accuracy\n",
    "print((less3 < more3).mean())\n",
    "\n",
    "less4 = s_model.predict(X_less_toxic)\n",
    "more4 = s_model.predict(X_more_toxic)\n",
    "# Validation Accuracy\n",
    "print((less4 < more4).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4815973a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
