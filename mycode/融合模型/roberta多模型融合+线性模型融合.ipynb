{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f099b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65a7751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from joblib import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7d33b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_val = pd.read_csv('/kaggle/input/jigsaw-toxic-severity-rating/validation_data.csv')\n",
    "df_val = pd.read_csv('/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv')\n",
    "df_sub = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/comments_to_score.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5f83ce",
   "metadata": {},
   "source": [
    "## 导入线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7c0cfa",
   "metadata": {},
   "source": [
    "## 思路2：切分roberta数据，融合的时候取出0.840+融合，保证数据不重复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "399761c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 7\n",
    "val_preds_arr1 = np.zeros((df_val.shape[0], n_folds))\n",
    "val_preds_arr2 = np.zeros((df_val.shape[0], n_folds))\n",
    "test_preds_arr = np.zeros((df_sub.shape[0], n_folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "924c8847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of features: 75032\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "Total number of features: 74635\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "Total number of features: 74452\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "Total number of features: 75816\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "Total number of features: 74327\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "Total number of features: 75102\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "Total number of features: 74475\n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n"
     ]
    }
   ],
   "source": [
    "for fld in range(n_folds):\n",
    "    pipeline = load('/home/xiaoguzai/模型/toxic-linear/withclean'+str(fld)+'.joblib')\n",
    "    \n",
    "    # What are the important features for toxicity\n",
    "\n",
    "    print('\\nTotal number of features:', len(pipeline['features'].get_feature_names()) )\n",
    "\n",
    "    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n",
    "                                  np.round(pipeline['clf'].coef_,2) )), \n",
    "                         key = lambda x:x[1], \n",
    "                         reverse=True)\n",
    "\n",
    "    #pprint(feature_wts[:30])\n",
    "    \n",
    "    print(\"\\npredict validation data \")\n",
    "    val_preds_arr1[:,fld] = pipeline.predict(df_val['less_toxic'])\n",
    "    val_preds_arr2[:,fld] = pipeline.predict(df_val['more_toxic'])\n",
    "    \n",
    "    print(\"\\npredict test data \")\n",
    "    test_preds_arr[:,fld] = pipeline.predict(df_sub['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32fc0753",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds_arr1c = np.zeros((df_val.shape[0], n_folds))\n",
    "val_preds_arr2c = np.zeros((df_val.shape[0], n_folds))\n",
    "test_preds_arrc = np.zeros((df_sub.shape[0], n_folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46ccd2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n"
     ]
    }
   ],
   "source": [
    "for fld in range(n_folds):\n",
    "    pipeline = load('/home/xiaoguzai/模型/toxic-linear/withoutclean'+str(fld)+'.joblib')\n",
    "    \n",
    "    # What are the important features for toxicity\n",
    "\n",
    "    #print('\\nTotal number of features:', len(pipeline['features'].get_feature_names()) )\n",
    "\n",
    "    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n",
    "                                  np.round(pipeline['clf'].coef_,2) )), \n",
    "                         key = lambda x:x[1], \n",
    "                         reverse=True)\n",
    "\n",
    "    #pprint(feature_wts[:30])\n",
    "    \n",
    "    print(\"\\npredict validation data \")\n",
    "    val_preds_arr1c[:,fld] = pipeline.predict(df_val['less_toxic'])\n",
    "    val_preds_arr2c[:,fld] = pipeline.predict(df_val['more_toxic'])\n",
    "    \n",
    "    print(\"\\npredict test data \")\n",
    "    test_preds_arrc[:,fld] = pipeline.predict(df_sub['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9253d979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n",
      "\n",
      "predict validation data \n",
      "\n",
      "predict test data \n"
     ]
    }
   ],
   "source": [
    "val_preds_arr1_ = np.zeros((df_val.shape[0], n_folds))\n",
    "val_preds_arr2_ = np.zeros((df_val.shape[0], n_folds))\n",
    "test_preds_arr_ = np.zeros((df_sub.shape[0], n_folds))\n",
    "\n",
    "for fld in range(n_folds):\n",
    "    pipeline = load('/home/xiaoguzai/模型/toxic-linear/withruddit'+str(fld)+'.joblib')\n",
    "    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n",
    "                                  np.round(pipeline['clf'].coef_,2) )), \n",
    "                         key = lambda x:x[1], \n",
    "                         reverse=True)\n",
    "\n",
    "    #pprint(feature_wts[:30])\n",
    "    print(\"\\npredict validation data \")\n",
    "    val_preds_arr1_[:,fld] = pipeline.predict(df_val['less_toxic'])\n",
    "    val_preds_arr2_[:,fld] = pipeline.predict(df_val['more_toxic'])\n",
    "\n",
    "    print(\"\\npredict test data \")\n",
    "    test_preds_arr_[:,fld] = pipeline.predict(df_sub['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c822c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Toxic data \n",
      "Validation Accuracy is 82.54\n",
      " Ruddit data \n",
      "Validation Accuracy is 73.49\n",
      " Toxic CLEAN data \n",
      "Validation Accuracy is 83.81\n"
     ]
    }
   ],
   "source": [
    "print(\" Toxic data \")\n",
    "p1 = val_preds_arr1.mean(axis=1)\n",
    "p2 = val_preds_arr2.mean(axis=1)\n",
    "\n",
    "print(f'Validation Accuracy is { np.round((p1 < p2).mean() * 100,2)}')\n",
    "\n",
    "print(\" Ruddit data \")\n",
    "p3 = val_preds_arr1_.mean(axis=1)\n",
    "p4 = val_preds_arr2_.mean(axis=1)\n",
    "\n",
    "print(f'Validation Accuracy is { np.round((p3 < p4).mean() * 100,2)}')\n",
    "\n",
    "print(\" Toxic CLEAN data \")\n",
    "p5 = val_preds_arr1c.mean(axis=1)\n",
    "p6 = val_preds_arr2c.mean(axis=1)\n",
    "\n",
    "print(f'Validation Accuracy is { np.round((p5 < p6).mean() * 100,2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87a2e846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find right weight linear\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.68, 0.13, 0.18999999999999995, 83.29),\n",
       " (0.69, 0.13, 0.18000000000000005, 83.29),\n",
       " (0.69, 0.12, 0.19000000000000006, 83.29),\n",
       " (0.67, 0.14, 0.18999999999999995, 83.27),\n",
       " (0.65, 0.16, 0.18999999999999997, 83.25)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Find right weight linear\")\n",
    "\n",
    "wts_acc = []\n",
    "for i in range(30,70,1):\n",
    "    for j in range(0,20,1):\n",
    "        w1 = i/100\n",
    "        w2 = (100 - i - j)/100\n",
    "        w3 = (1 - w1 - w2 )\n",
    "        p1_wt = w1*p1 + w2*p3 + w3*p5\n",
    "        p2_wt = w1*p2 + w2*p4 + w3*p6\n",
    "        wts_acc.append( (w1,w2,w3, \n",
    "                         np.round((p1_wt < p2_wt).mean() * 100,2))\n",
    "                      )\n",
    "sorted(wts_acc, key=lambda x:x[3], reverse=True)[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e94dcfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1,w2,w3,_ = sorted(wts_acc, key=lambda x:x[3], reverse=True)[0]\n",
    "#print(best_wts)\n",
    "\n",
    "p1_wt = w1*p1 + w2*p3 + w3*p5\n",
    "p2_wt = w1*p2 + w2*p4 + w3*p6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8832777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## roberta模型调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ee9ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pythonicforbert\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "import torch.nn as nn\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        if mask_ids is not None:\n",
    "            mask_ids = mask_ids[:,:,None].float()\n",
    "            output -= 1e-12*(1.0-mask_ids)\n",
    "        output = output[:,0]\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b49ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import RobertaTokenizer\n",
    "vocab_file = '/home/xiaoguzai/模型/roberta-english-base/vocab.json'\n",
    " \n",
    "merges_file = '/home/xiaoguzai/模型/roberta-english-base/merges.txt'\n",
    "tokenizer = RobertaTokenizer(vocab_file, merges_file)\n",
    "#tokenizer = RobertaTokenizer.from_pretrained('../input/roberta-base')\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0adedeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "roberta = get_model_function('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19ba3a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:02<00:00, 2201.68it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 2705.41it/s]\n",
      "100%|█████████████████████████████████████| 7537/7537 [00:02<00:00, 2751.98it/s]\n"
     ]
    }
   ],
   "source": [
    "lesstoxic_valid_dataset = TestDataset(df_val['less_toxic'],maxlen=500)\n",
    "lesstoxic_valid_loader = DataLoader(lesstoxic_valid_dataset,batch_size=16)\n",
    "moretoxic_valid_dataset = TestDataset(df_val['more_toxic'],maxlen=500)\n",
    "moretoxic_valid_loader = DataLoader(moretoxic_valid_dataset,batch_size=16)\n",
    "test_dataset = TestDataset(df_sub['text'],maxlen=500)\n",
    "test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7d3d6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.45it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.38it/s]\n",
      "100%|█████████████████████████████████████████| 472/472 [00:45<00:00, 10.36it/s]\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=0.8418643362463587seed=15.pth')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "less_toxic1,more_toxic1 = [],[]\n",
    "result_toxic1 = []\n",
    "for batch_token in tqdm(lesstoxic_valid_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        less_toxic1 = less_toxic1+current_point.cpu().numpy().tolist()\n",
    "\n",
    "for batch_token in tqdm(moretoxic_valid_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        more_toxic1 = more_toxic1+current_point.cpu().numpy().tolist()\n",
    "        \n",
    "for batch_token in tqdm(test_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        result_toxic1 = result_toxic1+current_point.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0094fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.31it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.31it/s]\n",
      "100%|█████████████████████████████████████████| 472/472 [00:45<00:00, 10.29it/s]\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=0.8416562630045776seed=4.pth')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "less_toxic2,more_toxic2 = [],[]\n",
    "result_toxic2 = []\n",
    "for batch_token in tqdm(lesstoxic_valid_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        less_toxic2 = less_toxic2+current_point.cpu().numpy().tolist()\n",
    "        \n",
    "for batch_token in tqdm(moretoxic_valid_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        more_toxic2 = more_toxic2+current_point.cpu().numpy().tolist()\n",
    "        \n",
    "for batch_token in tqdm(test_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        result_toxic2 = result_toxic2+current_point.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3182291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.31it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.31it/s]\n",
      "100%|█████████████████████████████████████████| 472/472 [00:45<00:00, 10.31it/s]\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=0.8410320432792343seed=5.pth')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "less_toxic3,more_toxic3 = [],[]\n",
    "result_toxic3 = []\n",
    "for batch_token in tqdm(lesstoxic_valid_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        less_toxic3 = less_toxic3+current_point.cpu().numpy().tolist()\n",
    "        \n",
    "for batch_token in tqdm(moretoxic_valid_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        more_toxic3 = more_toxic3+current_point.cpu().numpy().tolist()\n",
    "        \n",
    "for batch_token in tqdm(test_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        result_toxic3 = result_toxic3+current_point.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04074ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.30it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.31it/s]\n",
      "100%|█████████████████████████████████████████| 472/472 [00:45<00:00, 10.31it/s]\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=0.8401997503121099seed=1.pth')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "less_toxic4,more_toxic4 = [],[]\n",
    "result_toxic4 = []\n",
    "for batch_token in tqdm(lesstoxic_valid_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        less_toxic4 = less_toxic4+current_point.cpu().numpy().tolist()\n",
    "        \n",
    "for batch_token in tqdm(moretoxic_valid_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        more_toxic4 = more_toxic4+current_point.cpu().numpy().tolist()\n",
    "        \n",
    "for batch_token in tqdm(test_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        result_toxic4 = result_toxic4+current_point.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c409e2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 301/301 [00:27<00:00, 10.81it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:27<00:00, 11.14it/s]\n",
      "100%|█████████████████████████████████████████| 472/472 [00:43<00:00, 10.92it/s]\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=0.8395755305867666seed=8.pth')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "less_toxic5,more_toxic5 = [],[]\n",
    "result_toxic5 = []\n",
    "for batch_token in tqdm(lesstoxic_valid_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        less_toxic5 = less_toxic5+current_point.cpu().numpy().tolist()\n",
    "        \n",
    "for batch_token in tqdm(moretoxic_valid_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        more_toxic5 = more_toxic5+current_point.cpu().numpy().tolist()\n",
    "        \n",
    "for batch_token in tqdm(test_loader):\n",
    "    batch_token = batch_token.to(device)\n",
    "    with torch.no_grad():\n",
    "        current_point = model(batch_token)\n",
    "        current_point = current_point.squeeze(-1)\n",
    "        result_toxic5 = result_toxic5+current_point.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "759f171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "def calculate_data(i,j):\n",
    "    for k in range(0,31):\n",
    "        for u in range(0,31):\n",
    "            v = 100-i-j-k-u\n",
    "            for z in range(0,30):\n",
    "                z = z*0.1\n",
    "                less_toxic_wt = np.array(less_toxic1).dot(i)+np.array(less_toxic2).dot(j)+np.array(less_toxic3).dot(k)+\\\n",
    "                            np.array(less_toxic4).dot(u)+np.array(less_toxic5).dot(v)\n",
    "                less_toxic_wt = less_toxic_wt+z*p1_wt\n",
    "                #!!!注意这里的z是乘在哪个部位的 submission时保持一致\n",
    "                more_toxic_wt = np.array(more_toxic1).dot(i)+np.array(more_toxic2).dot(j)+np.array(more_toxic3).dot(k)+\\\n",
    "                            np.array(more_toxic4).dot(u)+np.array(more_toxic5).dot(v)\n",
    "                more_toxic_wt = more_toxic_wt+z*p2_wt\n",
    "                wts_acc.append((i,j,k,u,v,z,\n",
    "                        np.round((less_toxic_wt < more_toxic_wt).mean() * 100,2))\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5736efb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 31/31 [11:59:58<00:00, 1393.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43962 second\n"
     ]
    }
   ],
   "source": [
    "wts_acc = multiprocessing.Manager().list()\n",
    "#思路修正：1.各個相同的模型先分別進行融合，再進行最終的融合\n",
    "#嘗試方向2.對validation_data進行預訓練\n",
    "start_time = time.time()\n",
    "for i in tqdm(range(0,31)):\n",
    "    for j in range(0,31):\n",
    "        #pool.apply_async(calculate_data,(i,j,))\n",
    "        p = multiprocessing.Process(target=calculate_data,args=(i,j,))\n",
    "        p.start()\n",
    "        p.join()\n",
    "\n",
    "sorted(wts_acc, key=lambda x:x[6], reverse=True)[:5]\n",
    "#新的结果不能使用w1,w2,w3，因为w1,w2,w3要作为最后的结果内容\n",
    "print('%d second'%(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "688ec10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wts_acc = list(wts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99381d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w4,w5,w6,w7,w8,w,_ = sorted(wts_acc, key=lambda x:x[6], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d69f858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 = 0.680000,w2 = 0.130000,w3 = 0.190000,w4 = 17.000000,w5 = 4.000000\n",
      "\n",
      "w5 = %f,w6 = \n"
     ]
    }
   ],
   "source": [
    "print('w1 = %f,w2 = %f,w3 = %f,w4 = %f,w5 = %f\\n'%(w1,w2,w3,w4,w5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15ed9b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(17, 4, 30, 11, 38, 2.9000000000000004, 85.58),\n",
       " (17, 4, 30, 12, 37, 2.9000000000000004, 85.58),\n",
       " (17, 5, 30, 10, 38, 2.9000000000000004, 85.58),\n",
       " (18, 4, 30, 10, 38, 2.9000000000000004, 85.58),\n",
       " (18, 4, 30, 11, 37, 2.9000000000000004, 85.58)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(wts_acc, key=lambda x:x[6], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dbee727",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/jigsaw-toxic-severity-rating/sample_submission.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1005223/4010729398.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_submission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/jigsaw-toxic-severity-rating/sample_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_submission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest_preds_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest_preds_arr_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest_preds_arrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                          \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_toxic1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_toxic2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_toxic3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw6\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_toxic4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw7\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_toxic5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_toxic6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_submission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"submission.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/jigsaw-toxic-severity-rating/sample_submission.csv'"
     ]
    }
   ],
   "source": [
    "df_submission = pd.read_csv('/kaggle/input/jigsaw-toxic-severity-rating/sample_submission.csv')\n",
    "df_submission['score'] = w*w1*test_preds_arr.mean(axis=1) + w*w2*test_preds_arr_.mean(axis=1) + w*w3*test_preds_arrc.mean(axis=1) + \\\n",
    "                         np.dot(np.array(result_toxic1),w4) + np.dot(np.array(result_toxic2),w5) + np.dot(np.array(result_toxic3),w6) + \\\n",
    "                         np.dot(np.array(result_toxic4),w7) + np.dot(np.array(result_toxic5),w8) + np.dot(np.array(result_toxic6),w9)\n",
    "df_submission[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
