{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0624f0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 ROBERTA weights from: /home/xiaoguzai/模型/hated-roberta/pytorch_model.bin. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tclassifier.dense.weight\n",
      "\troberta.embeddings.position_ids\n",
      "\tclassifier.out_proj.bias\n",
      "\tclassifier.out_proj.weight\n",
      "\tclassifier.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:04<00:00, 3921.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:00<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 310.938171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:00<00:00, 4900.91it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:27<00:00, 10.93it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:00<00:00, 5019.77it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:26<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8033707865168539\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [10:50<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 255.506363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4122.34it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.38it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4168.84it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8302122347066168\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:50<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 251.624405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4251.19it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.33it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4119.68it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8358302122347067\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:51<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 250.321625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4201.74it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.31it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4201.95it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8310445276737412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 2\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = get_data(roberta,'/home/xiaoguzai/模型/hated-roberta/pytorch_model.bin')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa158a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 ROBERTA weights from: /home/xiaoguzai/模型/hated-roberta/pytorch_model.bin. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tclassifier.dense.weight\n",
      "\troberta.embeddings.position_ids\n",
      "\tclassifier.out_proj.bias\n",
      "\tclassifier.out_proj.weight\n",
      "\tclassifier.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:04<00:00, 3376.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:16<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 299.116730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3895.41it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.36it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3886.26it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8029546400332918\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:48<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 254.154221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3769.41it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.33it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3816.94it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8175197669579692\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:45<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 250.350464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4048.68it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:26<00:00, 11.15it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4094.96it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:27<00:00, 11.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8320848938826467\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [10:59<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 247.682816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4031.81it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:27<00:00, 10.83it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4110.59it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:27<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8331252600915522\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 3\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = get_data(roberta,'/home/xiaoguzai/模型/hated-roberta/pytorch_model.bin')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb2b65d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 ROBERTA weights from: /home/xiaoguzai/模型/hated-roberta/pytorch_model.bin. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tclassifier.dense.weight\n",
      "\troberta.embeddings.position_ids\n",
      "\tclassifier.out_proj.bias\n",
      "\tclassifier.out_proj.weight\n",
      "\tclassifier.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:05<00:00, 3245.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [10:59<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 299.718414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3960.91it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:27<00:00, 11.11it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3947.13it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:27<00:00, 10.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8289637952559301\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [10:58<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 255.439789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3675.58it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.34it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3833.14it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:27<00:00, 10.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8275072825634623\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:09<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 250.981110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3953.66it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:27<00:00, 11.14it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3987.76it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:27<00:00, 11.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8416562630045776\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [10:59<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 249.853729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4026.52it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.67it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3926.40it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:27<00:00, 11.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8331252600915522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 4\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = get_data(roberta,'/home/xiaoguzai/模型/hated-roberta/pytorch_model.bin')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a8d9159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 ROBERTA weights from: /home/xiaoguzai/模型/hated-roberta/pytorch_model.bin. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tclassifier.dense.weight\n",
      "\troberta.embeddings.position_ids\n",
      "\tclassifier.out_proj.bias\n",
      "\tclassifier.out_proj.weight\n",
      "\tclassifier.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:05<00:00, 3067.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:45<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 304.806366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3691.68it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.38it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3609.38it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8062838119017894\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:46<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 257.527496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3857.00it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3821.74it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8410320432792343\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:48<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 252.886185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3805.18it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3837.97it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8389513108614233\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:49<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 251.253738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3854.56it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.39it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3809.73it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8314606741573034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 5\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = get_data(roberta,'/home/xiaoguzai/模型/hated-roberta/pytorch_model.bin')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4745e5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 ROBERTA weights from: /home/xiaoguzai/模型/hated-roberta/pytorch_model.bin. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tclassifier.dense.weight\n",
      "\troberta.embeddings.position_ids\n",
      "\tclassifier.out_proj.bias\n",
      "\tclassifier.out_proj.weight\n",
      "\tclassifier.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:05<00:00, 3159.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:45<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 318.802124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3710.74it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.42it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3793.98it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.46254681647940077\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:44<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 267.732269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3828.85it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.41it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3773.40it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.449438202247191\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:46<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 259.432922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3817.07it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.49it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3785.47it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8127340823970037\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:49<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 252.858185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3789.39it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.42it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3792.76it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8277153558052435\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYOklEQVR4nO3df5BV533f8fcHJIHrVYRslB0ENEtHmBaLEZZ3JHncaS9SIyGcMc7E8UAdGxQS0hS1TqNJhdzJAJLVkaeWlSiVlawDFXITr4kTlx0JV0OR7mjcKZYgwiBQqDZibbEiIjEI51oxLeq3f9xn5Ru8y969P87l6vm8ZnbuOc95znme7+7d7z33OefeRxGBmZnlYVqnO2BmZsVx0jczy4iTvplZRpz0zcwy4qRvZpaRSzrdgQuZPXt29PX1Nbz/D3/4Q9797ne3rkNdILeYc4sXHHMumol5//79fxMRV4237aJO+n19fezbt6/h/cvlMqVSqXUd6gK5xZxbvOCYc9FMzJK+O9E2D++YmWXESd/MLCNO+mZmGXHSNzPLiJO+mVlGnPTNzDLipG9mlhEnfTOzjDjpm5ll5KL+RK7ZRPo2PgnAXUvOsTYtF2HkgY8U1pZZO/hM38wsI076ZmYZcdI3M8uIk76ZWUac9M3MMlJ30pc0XdILkp5I6wskfVvSsKSvSboslc9I68Npe1/NMe5J5Ucl3dbyaMzM7IKmcqb/GeClmvXPAw9FxDXAaWBdKl8HnE7lD6V6SFoMrALeDywHviRpenPdNzOzqagr6UuaB3wE+MO0LuBm4OupynbgY2l5ZVonbb8l1V8JDEbE2Yg4BgwDN7QgBjMzq1O9H876HeDfA5en9fcCb0TEubR+HJiblucCrwJExDlJZ1L9ucDemmPW7vM2SeuB9QC9vb2Uy+U6u/iTKpVKU/t3o1xivmtJ9anX+64fLxfhYvjd5vI3ruWYW2fSpC/p54CTEbFfUqnlPThPRAwAAwD9/f3RzLyYnlfznWttzSdyHzxU3AfLRz5ZKqytieTyN67lmFunnv+WDwMflbQCmAn8FPC7wCxJl6Sz/XnAaKo/CswHjku6BLgC+H5N+ZjafczMrACTjulHxD0RMS8i+qheiH06Ij4JPAN8PFVbA+xMy0NpnbT96YiIVL4q3d2zAFgIPNeySMzMbFLNvC++GxiU9DngBWBrKt8KfEXSMHCK6gsFEXFY0g7gCHAO2BARbzXRvpmZTdGUkn5ElIFyWn6Fce6+iYgfAb84wf73A/dPtZNmZtYa/kSumVlGnPTNzDLipG9mlhEnfTOzjDjpm5llxEnfzCwjTvpmZhlx0jczy4iTvplZRpz0zcwy4qRvZpYRJ30zs4w46ZuZZaS4KYfsnW3zFYU2NzKz+vh7PF5ou2bdzknfutqSaccYmbmpuAY31y6fKa5dsxbx8I6ZWUYmTfqSZkp6TtJ3JB2WtCWVPybpmKQD6WdpKpekhyUNSzoo6fqaY62R9HL6WTNBk9ZF+jY+SV+apNzMLn71DO+cBW6OiIqkS4FvSfpm2vZbEfH18+rfTnX+24XAjcCjwI2S3gNsAvqBAPZLGoqI060IxMzMJlfPxOgREZW0emn6iQvsshJ4PO23F5glaQ5wG7A7Ik6lRL8bWN5c983MbCoUcaH8nSpJ04H9wDXAIxFxt6THgA9RfSewB9gYEWclPQE8EBHfSvvuoTqJegmYGRGfS+W/DfxdRHzhvLbWA+sBent7Pzg4ONhwcJVKhZ6enob370ZFx3xotHoxc8m0Y4W1Wasy42p6zr7WkbaZs7Qjzfp5nYdmYl62bNn+iOgfb1tdd+9ExFvAUkmzgG9Iuha4B/gr4DJggGpiv7ehHv79tgbS8ejv749SqdTwscrlMs3s342KjnltGs8v9A6aGuVFWygd7UzbrO7M3Tt+XuehXTFP6e6diHgDeAZYHhEn0hDOWeC/ADekaqPA/Jrd5qWyicrNzKwg9dy9c1U6w0fSu4CfBf4ijdMjScDHgBfTLkPAp9NdPDcBZyLiBPAUcKukKyVdCdyayszMrCD1DO/MAbancf1pwI6IeELS05KuAgQcAP5Vqr8LWAEMA28CdwBExClJ9wHPp3r3RsSplkViZmaTmjTpR8RB4APjlN88Qf0ANkywbRuwbYp9NDOzFvEncs3MMuKkb2aWESd9M7OMOOmbmWXESd/MLCNO+mZmGXHSNzPLiJO+mVlGnPTNzDLipG9mlhEnfTOzjDjpm5llxEnfzCwjTvpmZhlx0jczy0g9M2fNlPScpO9IOixpSypfIOnbkoYlfU3SZal8RlofTtv7ao51Tyo/Kum2tkVlZmbjqudM/yxwc0RcBywFlqdpED8PPBQR1wCngXWp/jrgdCp/KNVD0mJgFfB+YDnwpTQbl5mZFWTSpJ8mP6+k1UvTTwA3A19P5dupzpMLsDKtk7bfkubRXQkMRsTZiDhGdTrFscnUzcysAPXMkUs6I98PXAM8Avwl8EZEnEtVjgNz0/Jc4FWAiDgn6Qzw3lS+t+awtfvUtrUeWA/Q29tLuVyeWkQ1KpVKU/t3o6JjvmtJ9SlQnralsDZrVWZcTXlRZ9qmQ88tP6/z0K6Y60r6EfEWsFTSLOAbwD9ueU9+3NYAMADQ398fpVKp4WOVy2Wa2b8bFR3z2o1PAjAyc1NhbdYqL9pC6Whn2mb1mY406+d1HtoV85Tu3omIN4BngA8BsySNvWjMA0bT8igwHyBtvwL4fm35OPuYmVkB6rl756p0ho+kdwE/C7xENfl/PFVbA+xMy0NpnbT96YiIVL4q3d2zAFgIPNeiOMzMrA71DO/MAbancf1pwI6IeELSEWBQ0ueAF4Ctqf5W4CuShoFTVO/YISIOS9oBHAHOARvSsJGZmRVk0qQfEQeBD4xT/grj3H0TET8CfnGCY90P3D/1bpqZWSv4E7lmZhlx0jczy4iTvplZRpz0zcwy4qRvZpYRJ30zs4w46ZuZZcRJ38wsI3V94ZqZ/aS+9GVz7TbywEcKacfy4DN9M7OMOOmbmWXEwztmDRqZ+S+LaWjzeeulnePVMquLz/TNzDLipG9mlhEnfTOzjDjpm5llpJ7pEudLekbSEUmHJX0mlW+WNCrpQPpZUbPPPZKGJR2VdFtN+fJUNixpY3tCMjOzidRz98454K6I+HNJlwP7Je1O2x6KiC/UVpa0mOoUie8Hrgb+h6T3pc2PUJ1j9zjwvKShiDjSikDMzGxy9UyXeAI4kZb/VtJLwNwL7LISGIyIs8CxNFfu2LSKw2maRSQNprpO+mZmBVFE1F9Z6gOeBa4FfhNYC/wA2Ef13cBpSf8Z2BsR/zXtsxX4ZjrE8oj4lVT+KeDGiLjzvDbWA+sBent7Pzg4ONhwcJVKhZ6enob370ZFx3xo9AwAS6YdK6zNWpUZV9Nz9rWOtN0plcuv8fM6A83EvGzZsv0R0T/etro/nCWpB/hT4Dci4geSHgXuAyI9Pgj8ckM9rBERA8AAQH9/f5RKpYaPVS6XaWb/blR0zGvT98+MzNxUWJu1you2UDrambY7pVza6ed1BtoVc11JX9KlVBP+H0XEnwFExOs1278MPJFWR4H5NbvPS2VcoNzMzApQz907ArYCL0XEF2vK59RU+3ngxbQ8BKySNEPSAmAh8BzwPLBQ0gJJl1G92DvUmjDMzKwe9Zzpfxj4FHBI0oFU9llgtaSlVId3RoBfA4iIw5J2UL1Aew7YEBFvAUi6E3gKmA5si4jDLYvEzMwmVc/dO98CNM6mXRfY537g/nHKd11oPzMzay9/ItfMLCNO+mZmGXHSNzPLiJO+mVlGnPTNzDLipG9mlhEnfTOzjDjpm5llxEnfzCwjTvpmZhlx0jczy4iTvplZRpz0zcwy4qRvZpYRJ30zs4w46ZuZZaSe6RLnS3pG0hFJhyV9JpW/R9JuSS+nxytTuSQ9LGlY0kFJ19cca02q/7KkNe0Ly8zMxlPPmf454K6IWAzcBGyQtBjYCOyJiIXAnrQOcDvVeXEXAuuBR6H6IgFsAm4EbgA2jb1QmJlZMSZN+hFxIiL+PC3/LfASMBdYCWxP1bYDH0vLK4HHo2ovMCtNon4bsDsiTkXEaWA3sLyVwZiZ2YUpIuqvLPUBzwLXAt+LiFmpXMDpiJgl6QnggTS3LpL2AHcDJWBmRHwulf828HcR8YXz2lhP9R0Cvb29HxwcHGw4uEqlQk9PT8P7d6OiYz40egaAJdOOFdZmrcqMq+k5+1pH2u6UyuXX+HmdgWZiXrZs2f6I6B9v26QTo4+R1AP8KfAbEfGDap6vioiQVP+rxwVExAAwANDf3x+lUqnhY5XLZZrZvxsVHfPajU8CMDJzU2Ft1iov2kLpaGfa7pRyaaef1xloV8x13b0j6VKqCf+PIuLPUvHradiG9HgylY8C82t2n5fKJio3M7OCTHqmn4ZutgIvRcQXazYNAWuAB9LjzpryOyUNUr1oeyYiTkh6CviPNRdvbwXuaU0Y9rYTB2DzysKaG5lZWFNm1gL1DO98GPgUcEjSgVT2WarJfoekdcB3gU+kbbuAFcAw8CZwB0BEnJJ0H/B8qndvRJxqRRAGfWmY5bHrOtwRM7uoTZr00wVZTbD5lnHqB7BhgmNtA7ZNpYNmZtY6/kSumVlGnPTNzDLipG9mlhEnfTOzjDjpm5llxEnfzCwjTvpmZhlx0jczy4iTvplZRpz0zcwy4qRvZpYRJ30zs4w46ZuZZcRJ38wsI076ZmYZqWfmrG3AzwEnI+LaVLYZ+FXgr1O1z0bErrTtHmAd8BbwbyPiqVS+HPhdYDrwhxHxQGtDMcvDodEzb89N3E4jD3yk7W1Y8eo5038MWD5O+UMRsTT9jCX8xcAq4P1pny9Jmi5pOvAIcDuwGFid6pqZWYHqmTnrWUl9dR5vJTAYEWeBY5KGgRvStuGIeAUgzZ+7Ejgy9S6bmVmjmhnTv1PSQUnbaiY7nwu8WlPneCqbqNzMzApUz8To43kUuA+I9Pgg8Mut6JCk9cB6gN7eXsrlcsPHqlQqTe3fTe5acg6AyoyrKS/a0uHeFCe3eAF6Z/z4791OF9P/Tk7/y2PaFXNDST8iXh9blvRl4Im0OgrMr6k6L5VxgfLzjz0ADAD09/dHqVRqpItA9UnbzP7dZOzC3mPXHaN0dFOHe1Oc8qItWcUL8HsLH+fBQ42er9Vv5JOltrdRr5z+l8e0K+aGhnckzalZ/XngxbQ8BKySNEPSAmAh8BzwPLBQ0gJJl1G92DvUeLfNzKwR9dyy+VWgBMyWdBzYBJQkLaU6vDMC/BpARByWtIPqBdpzwIaIeCsd507gKaq3bG6LiMOtDsYsB0umHWNkZgHvbjaPV3am/e1aW9Vz987qcYq3XqD+/cD945TvAnZNqXdmZtZS/kSumVlGnPTNzDLipG9mlhEnfTOzjDjpm5llxEnfzCwjTvpmZhlx0jczy4iTvplZRpz0zcwy4qRvZpYRJ30zs4w46ZuZZcRJ38wsI076ZmYZcdI3M8vIpElf0jZJJyW9WFP2Hkm7Jb2cHq9M5ZL0sKRhSQclXV+zz5pU/2VJa9oTjpmZXUg9Z/qPAcvPK9sI7ImIhcCetA5wO9V5cRcC64FHofoiQXWaxRuBG4BNYy8UZmZWnHqmS3xWUt95xSupzpsLsB0oA3en8scjIoC9kmalSdRLwO6IOAUgaTfVF5KvNh+CmRVm8xWdabe0szPtvgNNmvQn0BsRJ9LyXwG9aXku8GpNveOpbKLynyBpPdV3CfT29lIulxvsIlQqlab27yZ3LTkHQGXG1ZQXbelwb4qTW7yQacwZ/S+PaVfMjSb9t0VESIpWdCYdbwAYAOjv749SqdTwscrlMs3s303WbnwSgMeuO0bp6KYO96Y45UVbsooXMo25tDOb/+Ux7cpfjd6983oatiE9nkzlo8D8mnrzUtlE5WZmVqBGk/4QMHYHzhpgZ035p9NdPDcBZ9Iw0FPArZKuTBdwb01lZmZWoEmHdyR9leqF2NmSjlO9C+cBYIekdcB3gU+k6ruAFcAw8CZwB0BEnJJ0H/B8qnfv2EVdMzMrTj1376yeYNMt49QNYMMEx9kGbJtS78zMrKX8iVwzs4w46ZuZZcRJ38wsI03fp2/j6MCnFkdmVh/L5PWhHTObGp/pm5llxEnfzCwjHt4xs4vfiQOweWVn2t58pjPttonP9M3MMuKkb2aWESd9M7OMOOmbmWXESd/MLCNO+mZmGXHSNzPLiJN+i/WlaQvNzC5GTvpmZhlpKulLGpF0SNIBSftS2Xsk7Zb0cnq8MpVL0sOShiUdlHR9KwIwM7P6teJMf1lELI2I/rS+EdgTEQuBPWkd4HZgYfpZDzzagrbNzGwK2jG8sxLYnpa3Ax+rKX88qvYCsyTNaUP7ZmY2AVWntW1wZ+kYcBoI4A8iYkDSGxExK20XcDoiZkl6AnggIr6Vtu0B7o6Ifecdcz3VdwL09vZ+cHBwsOH+VSoVenp6Gt6/EYdGz7Bk2rFC26xVmXE1PWdf61j7RcstXnDMhZuztCPNNpO/li1btr9m9OXvafZbNv9pRIxK+mlgt6S/qN0YESFpSq8qETEADAD09/dHqVRquHPlcplm9m/E2o1PMjJzU6Ft1iov2kLpaOfaL1pu8YJjLtzqznzLZrvyV1PDOxExmh5PAt8AbgBeHxu2SY8nU/VRYH7N7vNSmZmZFaThpC/p3ZIuH1sGbgVeBIaANanaGmBnWh4CPp3u4rkJOBMRJxruuZmZTVkzwzu9wDeqw/ZcAvxxRPx3Sc8DOyStA74LfCLV3wWsAIaBN4E7mmjbzMwa0HDSj4hXgOvGKf8+cMs45QFsaLQ9MzNrnj+Ra2aWESd9M7OMeGJ0M7ML2XxFZ9ot7Zy8TgN8pm9mlhEnfTOzjDjpm5llxEnfzCwjTvpmZhlx0jczy4iTvplZRpz0zcwy4qRvZpYRJ30zs4w46ZuZZeSd/d07Jw7A5pWFNjkys9DmzMymxGf6ZmYZKTzpS1ou6aikYUkbi27fzCxnhSZ9SdOBR4DbgcXAakmLi+yDmVnOij7TvwEYjohXIuL/AINAsYPuZmYZU3Xq2oIakz4OLI+IX0nrnwJujIg7a+qsB9an1UXA0SaanA38TRP7d6PcYs4tXnDMuWgm5p+JiKvG23DR3b0TEQPAQCuOJWlfRPS34ljdIreYc4sXHHMu2hVz0cM7o8D8mvV5qczMzApQdNJ/HlgoaYGky4BVwFDBfTAzy1ahwzsRcU7SncBTwHRgW0QcbmOTLRkm6jK5xZxbvOCYc9GWmAu9kGtmZp3lT+SamWXESd/MLCNdn/Qn+1oHSTMkfS1t/7akvg50s6XqiPk3JR2RdFDSHkk/04l+tlK9X98h6RckhaSuv72vnpglfSL9rQ9L+uOi+9hqdTy3/6GkZyS9kJ7fKzrRz1aRtE3SSUkvTrBdkh5Ov4+Dkq5vutGI6NofqheD/xL4R8BlwHeAxefV+dfA76flVcDXOt3vAmJeBvyDtPzrOcSc6l0OPAvsBfo73e8C/s4LgReAK9P6T3e63wXEPAD8elpeDIx0ut9NxvzPgOuBFyfYvgL4JiDgJuDbzbbZ7Wf69Xytw0pge1r+OnCLJBXYx1abNOaIeCYi3kyre6l+HqKb1fv1HfcBnwd+VGTn2qSemH8VeCQiTgNExMmC+9hq9cQcwE+l5SuA1wrsX8tFxLPAqQtUWQk8HlV7gVmS5jTTZrcn/bnAqzXrx1PZuHUi4hxwBnhvIb1rj3pirrWO6plCN5s05vS2d35EPFlkx9qonr/z+4D3SfqfkvZKWl5Y79qjnpg3A78k6TiwC/g3xXStY6b6/z6pi+5rGKx1JP0S0A/88073pZ0kTQO+CKztcFeKdgnVIZ4S1Xdzz0paEhFvdLJTbbYaeCwiHpT0IeArkq6NiP/X6Y51i24/06/nax3eriPpEqpvCb9fSO/ao66vspD0L4D/AHw0Is4W1Ld2mSzmy4FrgbKkEapjn0NdfjG3nr/zcWAoIv5vRBwD/jfVF4FuVU/M64AdABHxv4CZVL+Y7J2q5V9d0+1Jv56vdRgC1qTljwNPR7pC0qUmjVnSB4A/oJrwu32cFyaJOSLORMTsiOiLiD6q1zE+GhH7OtPdlqjnuf3fqJ7lI2k21eGeVwrsY6vVE/P3gFsAJP0Tqkn/rwvtZbGGgE+nu3huAs5ExIlmDtjVwzsxwdc6SLoX2BcRQ8BWqm8Bh6leMFnVuR43r86Y/xPQA/xJumb9vYj4aMc63aQ6Y35HqTPmp4BbJR0B3gJ+KyK69l1snTHfBXxZ0r+jelF3bTefxEn6KtUX7tnpOsUm4FKAiPh9qtctVgDDwJvAHU232cW/LzMzm6JuH94xM7MpcNI3M8uIk76ZWUac9M3MMuKkb2aWESd9M7OMOOmbmWXk/wMif+w/uYW4VwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 6\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = get_data(roberta,'/home/xiaoguzai/模型/hated-roberta/pytorch_model.bin')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a6dee4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 ROBERTA weights from: /home/xiaoguzai/模型/hated-roberta/pytorch_model.bin. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tclassifier.dense.weight\n",
      "\troberta.embeddings.position_ids\n",
      "\tclassifier.out_proj.bias\n",
      "\tclassifier.out_proj.weight\n",
      "\tclassifier.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:05<00:00, 3067.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:45<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 294.233948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3702.21it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.42it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3592.33it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.813358302122347\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:45<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 255.965820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3603.78it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.39it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3685.16it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8245942571785269\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:46<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 252.820282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3750.44it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.38it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3639.97it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8266749895963379\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:48<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 252.290802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3650.16it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.37it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3625.34it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8322929671244278\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 7\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = get_data(roberta,'/home/xiaoguzai/模型/hated-roberta/pytorch_model.bin')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f10faf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 ROBERTA weights from: /home/xiaoguzai/模型/hated-roberta/pytorch_model.bin. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tclassifier.dense.weight\n",
      "\troberta.embeddings.position_ids\n",
      "\tclassifier.out_proj.bias\n",
      "\tclassifier.out_proj.weight\n",
      "\tclassifier.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:05<00:00, 3032.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:45<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 301.064728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3801.30it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.42it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3817.59it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8154390345401581\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:44<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 253.961578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3810.72it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.41it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3738.51it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8337494798168955\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:46<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 250.345459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3853.86it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.39it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3753.56it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8395755305867666\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:48<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 248.380295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3831.63it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3650.81it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8335414065751144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 8\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = get_data(roberta,'/home/xiaoguzai/模型/hated-roberta/pytorch_model.bin')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87a2fdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 ROBERTA weights from: /home/xiaoguzai/模型/hated-roberta/pytorch_model.bin. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tclassifier.dense.weight\n",
      "\troberta.embeddings.position_ids\n",
      "\tclassifier.out_proj.bias\n",
      "\tclassifier.out_proj.weight\n",
      "\tclassifier.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:05<00:00, 3109.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:44<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 305.746246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3729.34it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.41it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3631.87it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.782147315855181\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:44<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 256.282410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3813.09it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.41it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3784.53it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8354140657511444\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:46<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 252.417923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3774.37it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.50it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3694.89it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8331252600915522\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:48<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 250.599930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3738.68it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.38it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3678.83it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8374947981689554\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAARCklEQVR4nO3df6zddX3H8ee7VKgTpGjdDbbdLguVrdqo7AYxJttFNqiwUJKhKfNHa7o1cbi4jWzWLUtblQSzKZuJP9aNhmo2C3M/2lgXQoATsmVFYSgIpONKr9LaybSl25XJVvfeH+dz9aTeyz2395zv6eHzfCQ39/v9fH98Pu/ec1/ne7/f7/k2MhNJUh0WDXoAkqTmGPqSVBFDX5IqYuhLUkUMfUmqyOJBD+D5LFu2LEdHR095++9973u85CUv6d2ATnO11QvWXAtrnp8HH3zwO5n5ipmWndahPzo6ygMPPHDK27daLcbHx3s3oNNcbfWCNdfCmucnIr4x2zJP70hSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkVO60/kSrMZ3bIPgBvXnGBjme63yZuvbqQfqZ880pekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUka5DPyLOiIiHIuILZf6CiLg/IiYi4vaIOLO0n1XmJ8ry0Y59fKC0H4iIK3tejSTpec3nSP99wOMd8x8BbsnMC4FjwKbSvgk4VtpvKesREauB9cCrgbXAJyPijIUNX5I0H12FfkSsAK4G/rLMB/Bm4PNllV3AtWV6XZmnLL+8rL8O2J2Zz2XmQWACuKQHNUiSutTtUzb/FPh94Jwy/3Lgmcw8UeYPAcvL9HLgKYDMPBERx8v6y4H9Hfvs3OaHImIzsBlgZGSEVqvV5RB/3NTU1IK2HzY11XvjmvZLb+TFP5rut9Pl37amn/M0a+6dOUM/In4FeDozH4yI8Z6P4CSZuQPYATA2Npbj46feZavVYiHbD5ua6t3Y8Wjljz7SzBPCJ98+3kg/c6np5zzNmnunm9+WNwHXRMRVwBLgpcCfAUsjYnE52l8BHC7rHwZWAociYjFwLvDdjvZpndtIkhow5zn9zPxAZq7IzFHaF2Lvycy3A/cC15XVNgB7yvTeMk9Zfk9mZmlfX+7uuQBYBXypZ5VIkua0kL+L3w/sjogPAw8Bt5b2W4HPRsQEcJT2GwWZ+WhE3AE8BpwAbsjMHyygf0nSPM0r9DOzBbTK9JPMcPdNZn4feOss298E3DTfQUqSesNP5EpSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFWkmf9ySOqTNYsOMrlkazOdbTt5/ngz/Uo9ZOirN7ad22h3k0va31tsb7Rfadh5ekeSKmLoS1JFDH1JqoihL0kV8UKuFmR0yz7gRxdWJZ3ePNKXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKjJn6EfEkoj4UkR8NSIejYjtpf2CiLg/IiYi4vaIOLO0n1XmJ8ry0Y59faC0H4iIK/tWlSRpRt0c6T8HvDkzXwu8DlgbEZcCHwFuycwLgWPAprL+JuBYab+lrEdErAbWA68G1gKfjIgzeliLJGkOc4Z+tk2V2ReVrwTeDHy+tO8Cri3T68o8ZfnlERGlfXdmPpeZB4EJ4JJeFCFJ6s7iblYqR+QPAhcCnwC+DjyTmSfKKoeA5WV6OfAUQGaeiIjjwMtL+/6O3XZu09nXZmAzwMjICK1Wa34VdZiamlrQ9sNmEPXeuKb9Emgt2t5ov9OmznolrYsG0zcDem3V9roGa+6lrkI/M38AvC4ilgJ/D/xsz0fyo752ADsAxsbGcnx8/JT31Wq1WMj2w2YQ9W7csg+AySVbG+13Wuui7YwfGEzfXH98IN3W9roGa+6led29k5nPAPcCbwSWRsT0m8YK4HCZPgysBCjLzwW+29k+wzaSpAZ0c/fOK8oRPhHxYuCXgcdph/91ZbUNwJ4yvbfMU5bfk5lZ2teXu3suAFYBX+pRHZKkLnRzeud8YFc5r78IuCMzvxARjwG7I+LDwEPArWX9W4HPRsQEcJT2HTtk5qMRcQfwGHACuKGcNpIkNWTO0M/Mh4HXz9D+JDPcfZOZ3wfeOsu+bgJumv8wJUm94CdyJakihr4kVcTQl6SKGPqSVBFDX5Iq0tUnciX9uNHyaeR+m7z56kb6UR080pekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxA9nSadocsmvNdPRtpPmx/fMtJbUFY/0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFZkz9CNiZUTcGxGPRcSjEfG+0v6yiLgrIp4o388r7RERH4+IiYh4OCIu7tjXhrL+ExGxoX9lSZJm0s2R/gngxsxcDVwK3BARq4EtwN2ZuQq4u8wDvAVYVb42A5+C9psEsBV4A3AJsHX6jUKS1Iw5Qz8zj2Tmv5bp/wIeB5YD64BdZbVdwLVleh3wmWzbDyyNiPOBK4G7MvNoZh4D7gLW9rIYSdLzi8zsfuWIUeA+4DXANzNzaWkP4FhmLo2ILwA3Z+Y/lWV3A+8HxoElmfnh0v5HwH9n5p+c1Mdm2n8hMDIy8vO7d+8+5eKmpqY4++yzT3n7YTOIeh85fByANYsONtrvtKmzXsnZz31rIH0PytQ5F1b1uob6fpdhYTVfdtllD2bm2EzLFne7k4g4G/hb4Lcz8z/bOd+WmRkR3b97PI/M3AHsABgbG8vx8fFT3ler1WIh2w+bQdS7ccs+ACaXbG2032mti7YzfmAwfQ9Ka3xPVa9rqO93GfpXc1d370TEi2gH/l9l5t+V5m+X0zaU70+X9sPAyo7NV5S22dolSQ3p5u6dAG4FHs/Mj3Us2gtM34GzAdjT0f6uchfPpcDxzDwC3AlcERHnlQu4V5Q2SVJDujm98ybgncAjEfGV0vYHwM3AHRGxCfgG8Lay7IvAVcAE8CzwboDMPBoRHwK+XNb7YGYe7UURgtEt+7jttU/AtnWN9ju5pNHuJC3QnKFfLsjGLIsvn2H9BG6YZV87gZ3zGaAkqXf8RK4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVpJv/LlHSaeSRw8fZuGVf3/uZvPnqvveh5nmkL0kV8UhfGjJrFh1kcsnW/ne0baa24/3vV33lkb4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUkTlDPyJ2RsTTEfG1jraXRcRdEfFE+X5eaY+I+HhETETEwxFxccc2G8r6T0TEhv6UI0l6Pt0c6d8GrD2pbQtwd2auAu4u8wBvAVaVr83Ap6D9JgFsBd4AXAJsnX6jkCQ1Z87Qz8z7gKMnNa8DdpXpXcC1He2fybb9wNKIOB+4ErgrM49m5jHgLn78jUSS1GeLT3G7kcw8Uqb/HRgp08uBpzrWO1TaZmuXNEy2nTuYfsf3DKbfF6BTDf0fysyMiOzFYAAiYjPtU0OMjIzQarVOeV9TU1ML2n6Y3LjmBFNnvZLWRdsHPZRGWXMdavpdntavmk819L8dEedn5pFy+ubp0n4YWNmx3orSdhgYP6m9NdOOM3MHsANgbGwsx8fHZ1qtK61Wi4VsP0w2btnHba89yPiBrYMeSqNaF2235gq0xvdU87s8rV/5daq3bO4Fpu/A2QDs6Wh/V7mL51LgeDkNdCdwRUScVy7gXlHaJEkNmvNIPyI+R/sofVlEHKJ9F87NwB0RsQn4BvC2svoXgauACeBZ4N0AmXk0Ij4EfLms98HMPPnisCSpz+YM/cy8fpZFl8+wbgI3zLKfncDOeY1OktRTfiJXkipi6EtSRRZ8y6ZmMIB7mSeXQIu6buOTNH8e6UtSRTzSl3T6O/IV2Lau+X63HW++zz7zSF+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+j02umXfoIcgSbMy9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRRYPegCSdNradu7g+h7f05fdeqQvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVeWF/IvfIV2Dbuka7nFzSaHeSNC8e6UtSRQx9SapI46EfEWsj4kBETETElqb7l6SaNRr6EXEG8AngLcBq4PqIWN3kGCSpZk0f6V8CTGTmk5n5P8BuoNkrrZJUscjM5jqLuA5Ym5m/XubfCbwhM9/bsc5mYHOZvQg4sIAulwHfWcD2w6a2esGaa2HN8/PTmfmKmRacdrdsZuYOYEcv9hURD2TmWC/2NQxqqxesuRbW3DtNn945DKzsmF9R2iRJDWg69L8MrIqICyLiTGA9sLfhMUhStRo9vZOZJyLivcCdwBnAzsx8tI9d9uQ00RCprV6w5lpYc480eiFXkjRYfiJXkipi6EtSRYY+9Od6rENEnBURt5fl90fE6ACG2VNd1Py7EfFYRDwcEXdHxE8PYpy91O3jOyLiVyMiI2Lob+/rpuaIeFv5WT8aEX/d9Bh7rYvX9k9FxL0R8VB5fV81iHH2SkTsjIinI+JrsyyPiPh4+fd4OCIuXnCnmTm0X7QvBn8d+BngTOCrwOqT1vlN4NNlej1w+6DH3UDNlwE/UabfU0PNZb1zgPuA/cDYoMfdwM95FfAQcF6Z/8lBj7uBmncA7ynTq4HJQY97gTX/AnAx8LVZll8F/CMQwKXA/Qvtc9iP9Lt5rMM6YFeZ/jxweUREg2PstTlrzsx7M/PZMruf9uchhlm3j+/4EPAR4PtNDq5Puqn5N4BPZOYxgMx8uuEx9lo3NSfw0jJ9LvCtBsfXc5l5H3D0eVZZB3wm2/YDSyPi/IX0Oeyhvxx4qmP+UGmbcZ3MPAEcB17eyOj6o5uaO22ifaQwzOasufzZuzIz9zU5sD7q5uf8KuBVEfHPEbE/ItY2Nrr+6KbmbcA7IuIQ8EXgt5oZ2sDM9/d9TqfdYxjUOxHxDmAM+MVBj6WfImIR8DFg44CH0rTFtE/xjNP+a+6+iFiTmc8MclB9dj1wW2Z+NCLeCHw2Il6Tmf836IENi2E/0u/msQ4/XCciFtP+k/C7jYyuP7p6lEVE/BLwh8A1mflcQ2Prl7lqPgd4DdCKiEna5z73DvnF3G5+zoeAvZn5v5l5EPg32m8Cw6qbmjcBdwBk5r8AS2g/mOyFquePrhn20O/msQ57gQ1l+jrgnixXSIbUnDVHxOuBP6cd+MN+nhfmqDkzj2fmsswczcxR2tcxrsnMBwYz3J7o5rX9D7SP8omIZbRP9zzZ4Bh7rZuavwlcDhARP0c79P+j0VE2ay/wrnIXz6XA8cw8spAdDvXpnZzlsQ4R8UHggczcC9xK+0/ACdoXTNYPbsQL12XNfwycDfxNuWb9zcy8ZmCDXqAua35B6bLmO4ErIuIx4AfA72Xm0P4V22XNNwJ/ERG/Q/ui7sZhPoiLiM/RfuNeVq5TbAVeBJCZn6Z93eIqYAJ4Fnj3gvsc4n8vSdI8DfvpHUnSPBj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSL/D/b58c8bfTyfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 9\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = get_data(roberta,'/home/xiaoguzai/模型/hated-roberta/pytorch_model.bin')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0846df42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 ROBERTA weights from: /home/xiaoguzai/模型/hated-roberta/pytorch_model.bin. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tclassifier.dense.weight\n",
      "\troberta.embeddings.position_ids\n",
      "\tclassifier.out_proj.bias\n",
      "\tclassifier.out_proj.weight\n",
      "\tclassifier.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:05<00:00, 3065.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:44<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 309.206879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3727.04it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3738.03it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8116937161880982\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:44<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 255.110016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3763.74it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.39it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3712.25it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8279234290470245\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:46<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 251.109543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3773.97it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3761.48it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.83832709113608\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:49<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 250.724030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3830.59it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.39it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3739.06it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8335414065751144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 10\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = get_data(roberta,'/home/xiaoguzai/模型/hated-roberta/pytorch_model.bin')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec369dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 ROBERTA weights from: /home/xiaoguzai/模型/hated-roberta/pytorch_model.bin. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tclassifier.dense.weight\n",
      "\troberta.embeddings.position_ids\n",
      "\tclassifier.out_proj.bias\n",
      "\tclassifier.out_proj.weight\n",
      "\tclassifier.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:05<00:00, 3047.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:44<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 321.522858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3714.38it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3587.15it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.7890137328339576\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:44<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 260.124176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3645.74it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3617.39it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8114856429463171\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:47<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 255.685242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3720.92it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.46it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3700.10it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8304203079483978\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:49<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 253.605667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3618.16it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.38it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3657.34it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8335414065751144\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 11\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = get_data(roberta,'/home/xiaoguzai/模型/hated-roberta/pytorch_model.bin')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d88a1f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 ROBERTA weights from: /home/xiaoguzai/模型/hated-roberta/pytorch_model.bin. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tclassifier.dense.weight\n",
      "\troberta.embeddings.position_ids\n",
      "\tclassifier.out_proj.bias\n",
      "\tclassifier.out_proj.weight\n",
      "\tclassifier.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:05<00:00, 3031.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:44<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 306.347656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3708.39it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.41it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3694.38it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8289637952559301\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:44<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 258.836823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3761.57it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3696.95it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8308364544319601\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:47<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 254.118378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3840.88it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.44it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3721.33it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.83458177278402\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:49<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 253.367920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3788.09it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.36it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3713.38it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8349979192675822\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 12\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = get_data(roberta,'/home/xiaoguzai/模型/hated-roberta/pytorch_model.bin')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22c6e424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 ROBERTA weights from: /home/xiaoguzai/模型/hated-roberta/pytorch_model.bin. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tclassifier.dense.weight\n",
      "\troberta.embeddings.position_ids\n",
      "\tclassifier.out_proj.bias\n",
      "\tclassifier.out_proj.weight\n",
      "\tclassifier.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:05<00:00, 3032.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:44<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 294.857849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3794.37it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.41it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3712.32it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8277153558052435\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:44<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 255.002335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3801.60it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3786.13it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8314606741573034\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:46<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 251.476288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3802.70it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.49it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3737.34it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.83458177278402\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:48<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 250.628204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3782.44it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3675.23it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.83458177278402\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 13\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = get_data(roberta,'/home/xiaoguzai/模型/hated-roberta/pytorch_model.bin')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26444342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 ROBERTA weights from: /home/xiaoguzai/模型/hated-roberta/pytorch_model.bin. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tclassifier.dense.weight\n",
      "\troberta.embeddings.position_ids\n",
      "\tclassifier.out_proj.bias\n",
      "\tclassifier.out_proj.weight\n",
      "\tclassifier.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:05<00:00, 3013.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:44<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 284.485596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3745.08it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.42it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3682.46it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8314606741573034\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:44<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 253.175339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3784.84it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3714.59it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8256346233874324\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:47<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 251.196472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3792.71it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.44it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3801.05it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8312526009155222\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:49<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 249.162064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3739.74it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.39it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3779.17it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8393674573449854\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 14\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = get_data(roberta,'/home/xiaoguzai/模型/hated-roberta/pytorch_model.bin')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf7acd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 ROBERTA weights from: /home/xiaoguzai/模型/hated-roberta/pytorch_model.bin. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tclassifier.dense.weight\n",
      "\troberta.embeddings.position_ids\n",
      "\tclassifier.out_proj.bias\n",
      "\tclassifier.out_proj.weight\n",
      "\tclassifier.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:05<00:00, 3045.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:45<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 287.204102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3762.62it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.41it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3660.32it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8166874739908447\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:45<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 255.676025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3800.40it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.40it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3649.13it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8418643362463587\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:47<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 252.516663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3782.97it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.42it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3741.20it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8360382854764877\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:49<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 251.063309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3655.00it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.37it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3691.31it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8331252600915522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 15\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = get_data(roberta,'/home/xiaoguzai/模型/hated-roberta/pytorch_model.bin')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
