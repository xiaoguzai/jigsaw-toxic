{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "817e4d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 ROBERTA weights from: /home/xiaoguzai/模型/unbiased-toxic-roberta/pytorch_model.bin. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tclassifier.dense.bias\n",
      "\tclassifier.out_proj.weight\n",
      "\tclassifier.out_proj.bias\n",
      "\tclassifier.dense.weight\n",
      "\troberta.embeddings.position_ids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:04<00:00, 3794.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:44<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 834.275818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4532.43it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.35it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4644.65it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.7833957553058677\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:46<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 808.867615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3950.19it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.31it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3911.27it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8281315022888056\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:47<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 802.357727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3827.88it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.28it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3801.36it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8291718684977112\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▎                                   | 443/4075 [01:17<10:34,  5.72it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1029966/2913520591.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_compute_multilabel_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_token_id1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_token_id2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_label1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_label2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 1\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        #self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        self.fc1 = nn.Linear(config.embedding_size,128)\n",
    "        self.fc2 = nn.Linear(128,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        #output = self.fc(output)\n",
    "        output = self.fc1(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    #loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = get_data(roberta,'/home/xiaoguzai/模型/unbiased-toxic-roberta/pytorch_model.bin')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
