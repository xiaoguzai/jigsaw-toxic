{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efd6bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 NEZHA weights from: /home/xiaoguzai/模型/toxic-comment预训练结果/labeled_data+model_epoch=110.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.fc.weight\n",
      "\tmodule.fc.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:04<00:00, 3952.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:50<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 388.755157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4795.14it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.25it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:00<00:00, 4894.06it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8187682064086559\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:54<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 262.625580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4052.97it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.21it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4168.00it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8418643362463587\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:55<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 257.955994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4107.84it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.17it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4176.13it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8439450686641697\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:57<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 254.101868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4149.73it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.16it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 4089.76it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8245942571785269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXkUlEQVR4nO3dcZBd5X3e8e+DAMlFBIFFdoSkZtVBViubiYx3AI877RU0WCgdrzN1PFISI4iSTVNok4ZJLdzpSAIzg6exwSSYZB2pCNfRWnXiagfkMqrgDkOnAqQgA4KobJAcJMsosYSca2q1or/+cV/ZN/Ku9uy9d8/h8j6fmZ095z3vOe/7k3afPXvu2XsUEZiZWR7Oq3oCZmZWHoe+mVlGHPpmZhlx6JuZZcShb2aWkfOrnsC5zJ07N/r7+9ve/wc/+AEXXXRR9ybUA3KrObd6wTXnopOa9+7d+zcRcfl4297Rod/f38+ePXva3r9er1Or1bo3oR6QW8251QuuORed1Czp2xNt8+UdM7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMvKP/ItdsMi8eOckt6x6rZOxD9/58JeOadcJn+mZmGXHom5llxKFvZpYRh76ZWUYc+mZmGXHom5llpHDoS5oh6XlJj6b1RZKekTQm6WuSLkztM9P6WNre33KMO1P7AUkf7Xo1ZmZ2TlM50/8t4JWW9c8B90XElcAJYG1qXwucSO33pX5IWgqsAt4PrAC+JGlGZ9M3M7OpKBT6khYAPw/8cVoXcD3w9dRlC/DxtDyY1knbb0j9B4GRiDgVEQeBMeCaLtRgZmYFFf2L3PuBfwdcnNbfC7wZEafT+mFgflqeD7wOEBGnJZ1M/ecDu1uO2brPj0gaAoYA+vr6qNfrBaf4kxqNRkf796Lcau57D9xx1enJO06Dqv6dc/s/BtfcTZOGvqR/DhyLiL2Sal2fwVkiYhgYBhgYGIhOHobshym/+/3+V7fz+RereTeRQ79cq2Tc3P6PwTV3U5Hvlo8AH5O0EpgF/BTwRWCOpPPT2f4C4EjqfwRYCByWdD5wCfC9lvYzWvcxM7MSTHpNPyLujIgFEdFP84XYJyLil4EngU+kbmuA7Wl5NK2Ttj8REZHaV6W7exYBi4Fnu1aJmZlNqpPfiz8NjEj6LPA8sCm1bwK+ImkMOE7zBwURsV/SNuBl4DRwW0S83cH4ZmY2RVMK/YioA/W0/Brj3H0TET8EfnGC/e8B7pnqJM3MrDv8F7lmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhmp5u0J7V2nf91jlYx7x1WVDGvWs3ymb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEd+9Y11xaNYvVTLu7/NIJeOa9apJz/QlzZL0rKRvSdovaWNqf1jSQUn70sey1C5JD0gak/SCpKtbjrVG0qvpY80EQ5qZ2TQpcqZ/Crg+IhqSLgCelvTNtO13I+LrZ/W/iebzbxcD1wIPAddKugxYDwwAAeyVNBoRJ7pRiJmZTW7S0E8PNW+k1QvSR5xjl0HgkbTfbklzJM0DasDOiDgOIGknsALY2v70LXdXnXeQQ7PWVzT6yYrGNWufmtk8SSdpBrAXuBJ4MCI+Lelh4MM0fxPYBayLiFOSHgXujYin0767aD5EvQbMiojPpvb/APzviPi9s8YaAoYA+vr6PjQyMtJ2cY1Gg9mzZ7e9fy+qrOaj+8ofE2jMvILZp75TydjMW1bJsP66zkMnNS9fvnxvRAyMt63QC7kR8TawTNIc4BuSPgDcCXwXuBAYphnsd7U1w7871nA6HgMDA1Gr1do+Vr1ep5P9e1FlNW8YLH9MoL5kI7UDFZ3pr67mTN9f13mYrpqndMtmRLwJPAmsiIij0XQK+E/ANanbEWBhy24LUttE7WZmVpIid+9cns7wkfQe4OeAv0jX6ZEk4OPAS2mXUeDmdBfPdcDJiDgKPA7cKOlSSZcCN6Y2MzMrSZHLO/OALem6/nnAtoh4VNITki4HBOwD/mXqvwNYCYwBbwG3AkTEcUl3A8+lfnedeVHXzMzKUeTunReAD47Tfv0E/QO4bYJtm4HNU5yjmZl1id+GwcwsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDJS5MlZsyQ9K+lbkvZL2pjaF0l6RtKYpK9JujC1z0zrY2l7f8ux7kztByR9dNqqMjOzcRU50z8FXB8RPwssA1akxyB+DrgvIq4ETgBrU/+1wInUfl/qh6SlwCrg/cAK4EvpaVxmZlaSSUM/Pfy8kVYvSB8BXA98PbVvofmcXIDBtE7afkN6ju4gMBIRpyLiIM3HKZ55mLqZmZWgyDNySWfke4ErgQeBvwTejIjTqcthYH5ang+8DhARpyWdBN6b2ne3HLZ1n9axhoAhgL6+Pur1+tQqatFoNDravxdVVvOSjeWPCTRmXkG9orGp6GvLX9d5mK6aC4V+RLwNLJM0B/gG8A+7PpMfjzUMDAMMDAxErVZr+1j1ep1O9u9FldW8YbD8MYH6ko3UDqyvZGxWn6xkWH9d52G6ap7S3TsR8SbwJPBhYI6kMz80FgBH0vIRYCFA2n4J8L3W9nH2MTOzEhS5e+fydIaPpPcAPwe8QjP8P5G6rQG2p+XRtE7a/kRERGpfle7uWQQsBp7tUh1mZlZAkcs784At6br+ecC2iHhU0svAiKTPAs8Dm1L/TcBXJI0Bx2nesUNE7Je0DXgZOA3cli4bmZlZSSYN/Yh4AfjgOO2vMc7dNxHxQ+AXJzjWPcA9U5+mmZl1g/8i18wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMFHprZTP7Sf3rHqtk3IdXXFTJuPbu4DN9M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMTHr3jqSFwCNAHxDAcER8UdIG4NeBv05dPxMRO9I+dwJrgbeBfxMRj6f2FcAXgRnAH0fEvd0tx6w8h2b9UiXj1n/0ZFKzqStyy+Zp4I6I+HNJFwN7Je1M2+6LiN9r7SxpKc1HJL4fuAL475LelzY/SPMZu4eB5ySNRsTL3SjEzMwmV+RxiUeBo2n5byW9Asw/xy6DwEhEnAIOpmflnnms4lh6zCKSRlJfh76ZWUkUEcU7S/3AU8AHgN8BbgG+D+yh+dvACUl/AOyOiP+c9tkEfDMdYkVE/Fpq/xRwbUTcftYYQ8AQQF9f34dGRkbaLq7RaDB79uy29+9FldV8dF/5YwKNmVcw+9R3Khm7Ko2Lr/TXdQY6qXn58uV7I2JgvG2F/yJX0mzgT4HfjojvS3oIuJvmdf67gc8Dv9rWDFtExDAwDDAwMBC1Wq3tY9XrdTrZvxdVVvOGwfLHBOpLNlI7sL6SsatSr23313UGpqvmQqEv6QKagf/ViPgzgIh4o2X7l4FH0+oRYGHL7gtSG+doNzOzEkx6y6YkAZuAVyLiCy3t81q6/QLwUloeBVZJmilpEbAYeBZ4DlgsaZGkC2m+2DvanTLMzKyIImf6HwE+BbwoaV9q+wywWtIympd3DgG/ARAR+yVto/kC7Wngtoh4G0DS7cDjNG/Z3BwR+7tWiZmZTarI3TtPAxpn045z7HMPcM847TvOtZ+ZmU0v/0WumVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZaTIk7MWSnpS0suS9kv6rdR+maSdkl5Nny9N7ZL0gKQxSS9IurrlWGtS/1clrZm+sszMbDxFnpx1GrgjIv5c0sXAXkk7gVuAXRFxr6R1wDrg08BNNB+RuBi4FngIuFbSZcB6YIDm07b2ShqNiBPdLipnLx45yS3rHit93EOzSh/SzNpQ5MlZR4GjaflvJb0CzAcGgVrqtgWo0wz9QeCRiAhgt6Q56Xm6NWBnRBwHSD84VgBbu1hP9q467yCHZq2vehpm9g41pWv6kvqBDwLPAH3pBwLAd4G+tDwfeL1lt8OpbaJ2MzMrSZHLOwBImg38KfDbEfF96cePzY2IkBTdmJCkIWAIoK+vj3q93vaxGo1GR/v3osbMK6gv2Vj1NEqTW72Q6de1a+6aQqEv6QKagf/ViPiz1PyGpHkRcTRdvjmW2o8AC1t2X5DajvDjy0Fn2utnjxURw8AwwMDAQNRqtbO7FFav1+lk/15U33o/tQP5XN6pL9mYVb0A9dr2/L6uc/xenqaai9y9I2AT8EpEfKFl0yhw5g6cNcD2lvab01081wEn02Wgx4EbJV2a7vS5MbWZmVlJipzpfwT4FPCipH2p7TPAvcA2SWuBbwOfTNt2ACuBMeAt4FaAiDgu6W7gudTvrjMv6pqZWTmK3L3zNKAJNt8wTv8AbpvgWJuBzVOZoJmZdY//ItfMLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8tIkcclbpZ0TNJLLW0bJB2RtC99rGzZdqekMUkHJH20pX1FahuTtK77pZiZ2WSKnOk/DKwYp/2+iFiWPnYASFoKrALen/b5kqQZkmYADwI3AUuB1amvmZmVqMjjEp+S1F/weIPASEScAg5KGgOuSdvGIuI1AEkjqe/LU5+yWeaO7oMNg9WMveFkNeNa1xR5MPpEbpd0M7AHuCMiTgDzgd0tfQ6nNoDXz2q/dryDShoChgD6+vqo1+ttT7DRaHS0fy9qzLyC+pKNVU+jNLnVCxXXXNH3U5bfy9NUc7uh/xBwNxDp8+eBX+3GhCJiGBgGGBgYiFqt1vax6vU6nezfi+pb76d2YH3V0yhNfcnGrOqFimteXc2Zfpbfy9NUc1uhHxFvnFmW9GXg0bR6BFjY0nVBauMc7WZmVpK2btmUNK9l9ReAM3f2jAKrJM2UtAhYDDwLPAcslrRI0oU0X+wdbX/aZmbWjknP9CVtBWrAXEmHgfVATdIympd3DgG/ARAR+yVto/kC7Wngtoh4Ox3nduBxYAawOSL2d7sYMzM7tyJ376wep3nTOfrfA9wzTvsOYMeUZmdmZl3lv8g1M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwyMmnoS9os6Zikl1raLpO0U9Kr6fOlqV2SHpA0JukFSVe37LMm9X9V0prpKcfMzM6lyJn+w8CKs9rWAbsiYjGwK60D3ETzubiLgSHgIWj+kKD5mMVrgWuA9Wd+UJiZWXkmDf2IeAo4flbzILAlLW8BPt7S/kg07QbmpIeofxTYGRHHI+IEsJOf/EFiZmbTbNJn5E6gLyKOpuXvAn1peT7weku/w6ltovafIGmI5m8J9PX1Ua/X25wiNBqNjvbvRY2ZV1BfsrHqaZQmt3qh4pq33l/JsI2Lr8zve3ma8qvd0P+RiAhJ0Y3JpOMNA8MAAwMDUavV2j5WvV6nk/17UX3r/dQOrK96GqWpL9mYVb2Qac217fl9L09TfrV7984b6bIN6fOx1H4EWNjSb0Fqm6jdzMxK1G7ojwJn7sBZA2xvab853cVzHXAyXQZ6HLhR0qXpBdwbU5uZmZVo0ss7krYCNWCupMM078K5F9gmaS3wbeCTqfsOYCUwBrwF3AoQEccl3Q08l/rdFRFnvzhsZmbTbNLQj4jVE2y6YZy+Adw2wXE2A5unNDszM+uqjl/ItXFsuKS6sTO7k8XMpsZvw2BmlhGHvplZRnx5x8ze+Y7ugw2D1Yy94WQ1404Tn+mbmWXEoW9mlhGHvplZRhz6ZmYZceibmWXEoW9mlhGHvplZRhz6ZmYZceibmWXEoW9mlhGHvplZRjoKfUmHJL0oaZ+kPantMkk7Jb2aPl+a2iXpAUljkl6QdHU3CjAzs+K6caa/PCKWRcRAWl8H7IqIxcCutA5wE7A4fQwBD3VhbDMzm4LpuLwzCGxJy1uAj7e0PxJNu4E5Zx6ubmZm5VDzCYdt7iwdBE4AAfxRRAxLejMi5qTtAk5ExBxJjwL3RsTTadsu4NMRseesYw7R/E2Avr6+D42MjLQ9v0ajwezZs9vev21H95U/ZtKYeQWzT32nsvHLllu94JpLN29ZJcN2kl/Lly/f23L15e/o9P30/3FEHJH008BOSX/RujEiQtKUfqpExDAwDDAwMBC1Wq3tydXrdTrZv21Vve83UF+ykdqB9ZWNX7bc6gXXXLrV1byf/nTlV0eXdyLiSPp8DPgGcA3wxpnLNunzsdT9CLCwZfcFqc3MzErSduhLukjSxWeWgRuBl4BRYE3qtgbYnpZHgZvTXTzXAScj4mjbMzczsynr5PJOH/CN5mV7zgf+JCL+m6TngG2S1gLfBj6Z+u8AVgJjwFvArR2MbWZmbWg79CPiNeBnx2n/HnDDOO0B3NbueGZm1jn/Ra6ZWUYc+mZmGen0lk0zs3e3DZdUM25t++R92uAzfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy8u5+G4aj+yp9ipWZ2TuNz/TNzDLi0Dczy0jpoS9phaQDksYkrSt7fDOznJUa+pJmAA8CNwFLgdWSlpY5BzOznJV9pn8NMBYRr0XE/wFGAL/SamZWEjUfXVvSYNIngBUR8Wtp/VPAtRFxe0ufIWAorS4BDnQw5FzgbzrYvxflVnNu9YJrzkUnNf9MRFw+3oZ33C2bETEMDHfjWJL2RMRAN47VK3KrObd6wTXnYrpqLvvyzhFgYcv6gtRmZmYlKDv0nwMWS1ok6UJgFTBa8hzMzLJV6uWdiDgt6XbgcWAGsDki9k/jkF25TNRjcqs5t3rBNediWmou9YVcMzOrlv8i18wsIw59M7OM9HzoT/a2DpJmSvpa2v6MpP4KptlVBWr+HUkvS3pB0i5JP1PFPLup6Nt3SPoXkkJSz9/eV6RmSZ9M/9f7Jf1J2XPstgJf239f0pOSnk9f3yurmGe3SNos6ZiklybYLkkPpH+PFyRd3fGgEdGzHzRfDP5L4B8AFwLfApae1edfAX+YllcBX6t63iXUvBz4e2n5N3OoOfW7GHgK2A0MVD3vEv6fFwPPA5em9Z+uet4l1DwM/GZaXgocqnreHdb8T4CrgZcm2L4S+CYg4DrgmU7H7PUz/SJv6zAIbEnLXwdukKQS59htk9YcEU9GxFtpdTfNv4foZUXfvuNu4HPAD8uc3DQpUvOvAw9GxAmAiDhW8hy7rUjNAfxUWr4E+E6J8+u6iHgKOH6OLoPAI9G0G5gjaV4nY/Z66M8HXm9ZP5zaxu0TEaeBk8B7S5nd9ChSc6u1NM8UetmkNadfexdGxGNlTmwaFfl/fh/wPkn/Q9JuSStKm930KFLzBuBXJB0GdgD/upypVWaq3++Tese9DYN1j6RfAQaAf1r1XKaTpPOALwC3VDyVsp1P8xJPjeZvc09Juioi3qxyUtNsNfBwRHxe0oeBr0j6QET8v6on1it6/Uy/yNs6/KiPpPNp/kr4vVJmNz0KvZWFpH8G/HvgYxFxqqS5TZfJar4Y+ABQl3SI5rXP0R5/MbfI//NhYDQi/m9EHAT+F80fAr2qSM1rgW0AEfE/gVk035js3arrb13T66Ff5G0dRoE1afkTwBORXiHpUZPWLOmDwB/RDPxev84Lk9QcEScjYm5E9EdEP83XMT4WEXuqmW5XFPna/q80z/KRNJfm5Z7XSpxjtxWp+a+AGwAk/SOaof/Xpc6yXKPAzekunuuAkxFxtJMD9vTlnZjgbR0k3QXsiYhRYBPNXwHHaL5gsqq6GXeuYM3/EZgN/Jf0mvVfRcTHKpt0hwrW/K5SsObHgRslvQy8DfxuRPTsb7EFa74D+LKkf0vzRd1bevkkTtJWmj+456bXKdYDFwBExB/SfN1iJTAGvAXc2vGYPfzvZWZmU9Trl3fMzGwKHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZeT/A3bbtrueVgumAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 1\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "    \n",
    "from loader_pretrain_weights import load_pretrain_data\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = load_pretrain_data(roberta,'/home/xiaoguzai/模型/toxic-comment预训练结果/labeled_data+model_epoch=110.pth')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7887bdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 NEZHA weights from: /home/xiaoguzai/模型/toxic-comment预训练结果/labeled_data+model_epoch=120.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.fc.weight\n",
      "\tmodule.fc.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:04<00:00, 3341.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:52<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 395.381683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3797.02it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.22it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3763.78it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.787973366625052\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:53<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 263.020630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3690.21it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.20it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3668.60it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8393674573449854\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:55<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 258.122681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3808.21it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.15it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3735.48it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.83832709113608\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:56<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 254.438690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3704.12it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.13it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3702.24it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8250104036620891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 1\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "    \n",
    "from loader_pretrain_weights import load_pretrain_data\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = load_pretrain_data(roberta,'/home/xiaoguzai/模型/toxic-comment预训练结果/labeled_data+model_epoch=120.pth')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba357c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 NEZHA weights from: /home/xiaoguzai/模型/toxic-comment预训练结果/labeled_data+model_epoch=130.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.fc.weight\n",
      "\tmodule.fc.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:05<00:00, 3115.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:51<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 392.810272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3792.09it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.22it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3736.84it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.7942155638784852\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:53<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 263.355438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3654.05it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.19it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3628.10it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8272992093216812\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:54<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 258.540161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3741.97it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.15it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3729.90it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8408239700374532\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:57<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 254.713120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3733.15it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.13it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3633.00it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8202247191011236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 1\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "    \n",
    "from loader_pretrain_weights import load_pretrain_data\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = load_pretrain_data(roberta,'/home/xiaoguzai/模型/toxic-comment预训练结果/labeled_data+model_epoch=130.pth')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a25877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 NEZHA weights from: /home/xiaoguzai/模型/toxic-comment预训练结果/labeled_data+model_epoch=140.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.fc.weight\n",
      "\tmodule.fc.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:05<00:00, 3069.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:52<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 396.548401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3712.00it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.23it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3718.58it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8168955472326259\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:54<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 263.652130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3696.63it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.19it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3733.80it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8339575530586767\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:55<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 258.196716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3767.96it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.14it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3715.06it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8433208489388264\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:57<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 254.321701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3730.61it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.14it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3721.66it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8264669163545568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 1\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "    \n",
    "from loader_pretrain_weights import load_pretrain_data\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = load_pretrain_data(roberta,'/home/xiaoguzai/模型/toxic-comment预训练结果/labeled_data+model_epoch=140.pth')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f1f18f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 NEZHA weights from: /home/xiaoguzai/模型/toxic-comment预训练结果/labeled_data+model_epoch=160.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.fc.weight\n",
      "\tmodule.fc.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:05<00:00, 3037.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:52<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 392.458313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3696.24it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.22it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3691.63it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8104452767374116\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:54<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 263.404785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3685.24it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.19it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3650.09it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8356221389929255\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:55<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 258.247101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3701.90it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.15it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3681.05it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8393674573449854\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:57<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 254.469742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3704.86it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.14it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3676.15it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8216812317935913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 1\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "    \n",
    "from loader_pretrain_weights import load_pretrain_data\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = load_pretrain_data(roberta,'/home/xiaoguzai/模型/toxic-comment预训练结果/labeled_data+model_epoch=160.pth')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8def4687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 NEZHA weights from: /home/xiaoguzai/模型/toxic-comment预训练结果/labeled_data+model_epoch=170.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.fc.weight\n",
      "\tmodule.fc.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:05<00:00, 3039.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:52<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 399.908936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3688.42it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.22it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3617.09it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8100291302538494\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:54<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 263.285126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3666.60it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.19it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3653.31it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8360382854764877\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:55<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 258.376404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3698.21it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.16it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3645.88it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8458177278401997\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:57<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 254.424301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3708.21it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.14it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3636.81it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8343736995422388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 1\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "    \n",
    "from loader_pretrain_weights import load_pretrain_data\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = load_pretrain_data(roberta,'/home/xiaoguzai/模型/toxic-comment预训练结果/labeled_data+model_epoch=170.pth')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "727e9c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 NEZHA weights from: /home/xiaoguzai/模型/toxic-comment预训练结果/labeled_data+model_epoch=180.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.fc.weight\n",
      "\tmodule.fc.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:05<00:00, 2969.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:52<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 395.573944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3667.29it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.22it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3646.06it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8135663753641281\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:54<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 262.879517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3661.71it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.19it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3517.55it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8395755305867666\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:54<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 258.207214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3696.16it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.11it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3553.32it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8478984602580109\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:56<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 254.221863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3669.18it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.14it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3653.13it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8252184769038702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 1\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "    \n",
    "from loader_pretrain_weights import load_pretrain_data\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = load_pretrain_data(roberta,'/home/xiaoguzai/模型/toxic-comment预训练结果/labeled_data+model_epoch=180.pth')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1e9b71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n",
      "Done loading 197 NEZHA weights from: /home/xiaoguzai/模型/toxic-comment预训练结果/labeled_data+model_epoch=190.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.fc.weight\n",
      "\tmodule.fc.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16299/16299 [00:05<00:00, 3188.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:50<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 398.052979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3830.37it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.32it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3769.69it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8123179359134415\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:47<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 262.981720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3900.80it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.35it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3485.55it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:29<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8420724094881398\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4075/4075 [11:46<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 258.053986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3962.38it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:28<00:00, 10.65it/s]\n",
      "100%|█████████████████████████████████████| 4806/4806 [00:01<00:00, 3910.83it/s]\n",
      "100%|█████████████████████████████████████████| 301/301 [00:27<00:00, 10.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_point = \n",
      "0.8456096545984186\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏                                        | 22/4075 [00:03<11:40,  5.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5663/937927264.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mbatch_label2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_label2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_compute_multilabel_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_token_id1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_token_id2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_label1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_label2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5663/937927264.py\u001b[0m in \u001b[0;36mtrain_compute_multilabel_loss\u001b[0;34m(x1, x2, model, label1, label2)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_compute_multilabel_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mlogit1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mlogit2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mlogitmargin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlogit2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mlabelmargin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlabel2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5663/937927264.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mmask_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m#英文roberta padding=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pythonicforbert/roberta/roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m#outputs = self.bert_encoder_layer[0](outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_ndx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroberta_encoder_layer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_ndx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0;31m#print('outputs1 = ')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m#print(outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pythonicforbert/roberta/roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, masks, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pythonicforbert/roberta/roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, mask, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_per_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 1\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "    \n",
    "from loader_pretrain_weights import load_pretrain_data\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = load_pretrain_data(roberta,'/home/xiaoguzai/模型/toxic-comment预训练结果/labeled_data+model_epoch=190.pth')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d4bc71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
