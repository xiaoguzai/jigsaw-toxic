{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efd6bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pythonicforbert in /home/xiaoguzai/.local/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/xiaoguzai/.local/lib/python3.9/site-packages (from pythonicforbert) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/xiaoguzai/.local/lib/python3.9/site-packages (from torch>=1.5.0->pythonicforbert) (3.10.0.2)\n",
      "(16299195, 4)\n",
      "(16299, 4)\n",
      "cuda:0\n",
      "16299\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file for bertat '/home/xiaoguzai/模型/toxic-comment second pretrain sequence/labeled_data+model_epoch=175.pth'If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/程序/kaggle有毒评论对比比赛/loader_pretrain_weights.py\u001b[0m in \u001b[0;36mload_pretrain_data\u001b[0;34m(model, resolved_archive_file)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/xiaoguzai/模型/toxic-comment second pretrain sequence/labeled_data+model_epoch=175.pth'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11110/2594435401.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mroberta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRoberta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mroberta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pretrain_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroberta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/home/xiaoguzai/模型/toxic-comment second pretrain sequence/labeled_data+model_epoch=175.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroberta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/程序/kaggle有毒评论对比比赛/loader_pretrain_weights.py\u001b[0m in \u001b[0;36mload_pretrain_data\u001b[0;34m(model, resolved_archive_file)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mmodel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             raise OSError(\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0;34mf\"Unable to load weights from pytorch checkpoint file for bert\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;34mf\"at '{resolved_archive_file}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for bertat '/home/xiaoguzai/模型/toxic-comment second pretrain sequence/labeled_data+model_epoch=175.pth'If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 1\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "    \n",
    "from loader_pretrain_weights import load_pretrain_data\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = load_pretrain_data(roberta,'/home/xiaoguzai/模型/toxic-comment second pretrain sequence/labeled_data+model_epoch=175.pth')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 1\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "    \n",
    "from loader_pretrain_weights import load_pretrain_data\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = load_pretrain_data(roberta,import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 1\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "    \n",
    "from loader_pretrain_weights import load_pretrain_data\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = load_pretrain_data(roberta,'/home/xiaoguzai/模型/toxic-comment second pretrain sequence/labeled_data+model_epoch=175.pth')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth'))\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6676fa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 1\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "    \n",
    "from loader_pretrain_weights import load_pretrain_data\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = load_pretrain_data(roberta,import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random_seed = 1\n",
    "frac_1 = 0.001\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(random_seed)\n",
    "\n",
    "!pip install pythonicforbert\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import gc \n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth=300\n",
    "\n",
    "df_ = pd.read_csv(\"/home/xiaoguzai/数据/kaggle有毒评论对比/ruddit-pair/ruddit_pairs.csv\")  # create a dateframe based on a file\n",
    "\n",
    "print(df_.shape) \n",
    "\n",
    "df_ = df_.sample(frac=frac_1,random_state=random_seed).sample(frac=1,random_state=random_seed)\n",
    "print(df_.shape)\n",
    "\n",
    "df_1 = df_[['txt1','sc1']].rename(columns={'txt1':'txt','sc1':'y'})\n",
    "df_2 = df_[['txt2','sc2']].rename(columns={'txt2':'txt','sc2':'y'})\n",
    "\n",
    "min_data = min(df_1.y.min(),df_2.y.min())\n",
    "max_data = max(df_2.y.max(),df_2.y.max())\n",
    "df_1['y'] = (df_1['y']-min_data)/(max_data-min_data)\n",
    "df_2['y'] = (df_2['y']-min_data)/(max_data-min_data)\n",
    "\n",
    "df_1.y.hist()\n",
    "df_2.y.hist()\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from pythonicforbert import get_model_function,FullTokenizer\n",
    "with open('/home/xiaoguzai/数据/kaggle有毒评论对比/roberta-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "    \n",
    "import torch.nn as nn\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        mask_ids = torch.not_equal(input_ids,1)\n",
    "        #英文roberta padding=1\n",
    "        output = self.model(input_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def train_compute_multilabel_loss(x1,x2,model,label1,label2):\n",
    "    logit1 = model(x1)\n",
    "    logit2 = model(x2)\n",
    "    logitmargin = logit1-logit2\n",
    "    labelmargin = label1-label2\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    marginloss = loss_fn(logitmargin,labelmargin)\n",
    "    return marginloss\n",
    "\n",
    "#tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-base\")\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text1,text2,maxlen,label1,label2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.maxlen = maxlen\n",
    "        self.label1 = label1\n",
    "        self.label2 = label2\n",
    "        token_id1,token_id2 = [],[]\n",
    "        for index in tqdm(range(len(self.text1))):\n",
    "            current_text1 = text1[index]\n",
    "            current_id = tokenizer(current_text1)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id1.append(current_id)\n",
    "            current_text2 = text2[index]\n",
    "            current_id = tokenizer(current_text2)['input_ids']\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id2.append(current_id)\n",
    "        self.tensors = [torch.tensor(token_id1),torch.tensor(token_id2),\\\n",
    "                        torch.tensor(label1),torch.tensor(label2)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label1)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 1):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分,\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "#易错点：testdataset的内容需要跟classificationdataset的内容保持一致\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_id = []\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_id = tokenizer(current_text)['input_ids']\n",
    "            #roberta begin:0,end:2\n",
    "            #current_id = tokenizer.convert_tokens_to_ids(current_token)\n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_id.append(current_id)\n",
    "        self.token_id = token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        #return [tuple(tensor[index] for tensor in self.tensors)]\n",
    "        return self.token_id[index]\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding=1):\n",
    "        #英文roberta padding=1\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length-1]+[inputs[-1]]\n",
    "        #保留[sep]标志部分\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "    \n",
    "valid_data_df = pd.read_csv(\"/home/xiaoguzai/数据/Kaggle Jigsaw Rate Severity of Toxic Comments/validation_true.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "bestpoint = []\n",
    "n_folds = 6\n",
    "#先单模单折\n",
    "\n",
    "r\"\"\"\n",
    "比较l1loss和mseloss的不同结果\n",
    "比较9e-6和2e-5的学习率\n",
    "\"\"\"\n",
    "\n",
    "less_toxic,more_toxic = valid_data_df['less_toxic'].values.tolist(),valid_data_df['more_toxic'].values.tolist()\n",
    "#print('less_toxic = ')\n",
    "#print(less_toxic[0:10])\n",
    "#print('more_toxic = ')\n",
    "#print(more_toxic[0:10])\n",
    "for current_slit in range(n_folds):\n",
    "    bestpoint.append(0.0)\n",
    "    \n",
    "from loader_pretrain_weights import load_pretrain_data\n",
    "for current_split in range(1):\n",
    "    train_text1,train_text2,train_label1,train_label2 = [],[],[],[]\n",
    "    #单模单折所有数据作为训练集\n",
    "    #下面部分数据可能加入了\n",
    "    train_text1 = train_text1+df_1['txt'].values.tolist()\n",
    "    train_text2 = train_text2+df_2['txt'].values.tolist()\n",
    "    train_label1 = train_label1+df_1['y'].values.tolist()\n",
    "    train_label2 = train_label2+df_2['y'].values.tolist()\n",
    "    \n",
    "    #加入ruddit_data进行训练\n",
    "    print(len(train_text1))\n",
    "    \n",
    "    #def __init__(self,text,maxlen,labels)\n",
    "    \n",
    "    #加载官方bert模型\n",
    "    Roberta,robertaconfig,get_data = get_model_function('roberta-base')\n",
    "    config = robertaconfig(**json_data)\n",
    "    config.with_mlm = False\n",
    "    roberta = Roberta(config)\n",
    "    roberta = load_pretrain_data(roberta,'/home/xiaoguzai/模型/toxic-comment second pretrain sequence/labeled_data+model_epoch=185.pth')\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth'))\n",
    "    model = ClassificationModel(roberta,config,1)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(text1=train_text1,text2=train_text2,\\\n",
    "                                          maxlen=500,label1=train_label1,label2=train_label2)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        #model = torch.load('/home/xiaoguzai/程序/kaggle有毒评论对比比赛/best_score=20261.0split=0.pth')\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.L1Loss(reduce=True,size_average=True)\n",
    "        \n",
    "\n",
    "        for batch_token_id1,batch_token_id2,batch_label1,batch_label2 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            batch_token_id1 = batch_token_id1.to(device)\n",
    "            batch_token_id2 = batch_token_id2.to(device)\n",
    "            batch_label1 = batch_label1.to(device)\n",
    "            batch_label2 = batch_label2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_compute_multilabel_loss(batch_token_id1,batch_token_id2,model,batch_label1,batch_label2)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        less_toxic_point,more_toxic_point = [],[]\n",
    "        right = []\n",
    "        test_dataset = TestDataset(less_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                less_toxic_point = less_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        test_dataset = TestDataset(more_toxic,maxlen=500)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "        for batch_token in tqdm(test_loader):\n",
    "            batch_token = batch_token.to(device)\n",
    "            with torch.no_grad():\n",
    "                result_point = model(batch_token)\n",
    "                more_toxic_point = more_toxic_point+result_point.cpu().numpy().tolist()\n",
    "        right_point = sum([less_toxic_point[index] < more_toxic_point[index] for index in range(len(less_toxic_point))])\n",
    "        right_point = right_point/len(less_toxic_point)\n",
    "        print('right_point = ')\n",
    "        print(right_point)\n",
    "        if right_point >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = right_point\n",
    "            torch.save(model,'./best_score='+str(bestpoint[current_split])+'seed='+str(random_seed)+'.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
